<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://danieljunghans.github.io/al-folio/feed.xml" rel="self" type="application/atom+xml" /><link href="https://danieljunghans.github.io/al-folio/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-10-16T01:59:07+00:00</updated><id>https://danieljunghans.github.io/al-folio/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Liquidity Determinants and Trends of Michigan Banks</title><link href="https://danieljunghans.github.io/al-folio/blog/2023/Liquidity/" rel="alternate" type="text/html" title="Liquidity Determinants and Trends of Michigan Banks" /><published>2023-10-14T08:58:00+00:00</published><updated>2023-10-14T08:58:00+00:00</updated><id>https://danieljunghans.github.io/al-folio/blog/2023/Liquidity</id><content type="html" xml:base="https://danieljunghans.github.io/al-folio/blog/2023/Liquidity/"><![CDATA[<p style="text-align: left;"><font size="+3"><b>Introduction</b></font></p>

<p align="justify">
The objective of this article is to examine bank specific factors that determine the level of liquid assets held by state chartered commercial banks in Michigan.  Bank specific factors include bank size, profitability, capital adequacy, deposits, loans, and asset quality.  This article also aims to explore ongoing liquidity trends impacting these financial institutions.  This study consists of balanced panel data of 59 state chartered commercial banks from December 31st, 2019, until June 30th, 2023.   
</p>
<p align="justify">
Under the fractional reserve banking system, banks only hold a fraction of their deposit liabilities in liquid assets to meet anticipated liquidity needs.  Bank management is responsible for determining the appropriate level of liquid assets required to meet the demands of depositors and creditors while allowing for the greatest profitability.  Developing a better understanding of the determinants of liquid assets will give key insights into liquidity trends.  
</p>

<p style="text-align: left;"><font size="+3"><b>Literature Review</b></font></p>

<p align="justify">
In 1999, an analysis of the Mexican banking system found that capital has a significant positive effect on liquid assets and the total asset size of the institutions (in relation to the total assets of the banking system) has no statistically significant effect on any measure of liquid assets.  This study also theorized that banks with relatively more demand deposits have more liquid assets but found that the level of demand deposits relative to total assets has a significant negative effect on securities and liquid assets but not the level of cash.  This finding could suggest that banks consider demand deposits to be a very stable liability, or the variance of deposit withdraws declines as deposits increase and the deposit population becomes more diverse (Alger &amp; Alger, 1999). 
</p>
<p align="justify">
The relationship between liquid assets and bank size was further examined in a 2015 study of 18 Tunisian banks.  This study used the natural logarithm of total assets as a measure of bank size and also found that bank size does not have a significant impact on bank liquidity supporting the findings of.  Additional variables in this study include return on assets (ROA), return on equity (ROE), net interest margin (NIM), total loans, operating expenses, total deposits, financial expenses, equity, inflation rate, and growth rate of GDP.  ROA, ROE, capital, operating expenses, growth rate of GDP, and  inflation rate were found to have a significant impact on bank liquidity.  Total loans, financial costs, and total deposits did not have a significant impact on bank liquidity (Moussa, 2015). 
</p>
<p align="justify">
Macroeconomic as well as bank specific determinants of liquidity of Indian banks were studied by Singh and Sharma (2016).  Bank specific determinants studied in this paper include bank size, profitability, cost of funding, capital adequacy, and deposits.  Macroeconomic determinants of liquidity considered include GDP, inflation, and unemployment.  This study found that bank size and GDP have a negative relationship with liquidity at a 5 percent confidence interval while profitability, deposits, inflation, and capital adequacy have a positive impact.  
</p>

<p style="text-align: left;"><font size="+3"><b>Specification of Variables</b></font></p>

<p align="justify">
The dependent variable in this article will be liquidity which will be measured through two liquidity ratios provided by Vodov’a (2011).  Fixed and random effect regression will be performed for both measures of liquidity.  The first measure of liquidity is net loans as a fraction of total assets (Liq1).  The ratio Liq1 indicates the percentage of bank assets which are invested in illiquid loans.  The higher this ratio is, the fewer liquid assets a bank possesses.  The second measure of liquidity is liquid assets as a fraction of total assets (Liq2).  The ratio Liq2 indicates the percentage of bank assets which are liquid.  A high ratio may indicate an inefficiency since liquid assets yield less income than illiquid assets.  Liquid assets consist of interest as well as non-interest bearing bank balances,  securities sold under agreement to repurchase, federal funds sold, and total securities less pledged securities.
</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Independent Variable</th>
      <th style="text-align: center">Proxy/Measurement</th>
      <th style="text-align: center">Notation</th>
      <th style="text-align: right">Expected Effect</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Bank Size</td>
      <td style="text-align: center">Natural log of total assets</td>
      <td style="text-align: center">SIZE</td>
      <td style="text-align: right">Negative</td>
    </tr>
    <tr>
      <td style="text-align: left">Profitability</td>
      <td style="text-align: center">Return on assets</td>
      <td style="text-align: center">ROA</td>
      <td style="text-align: right">Negative</td>
    </tr>
    <tr>
      <td style="text-align: left">Deposits</td>
      <td style="text-align: center">Deposits over total assets</td>
      <td style="text-align: center">DEP</td>
      <td style="text-align: right">Positive</td>
    </tr>
    <tr>
      <td style="text-align: left">Capital Adequacy</td>
      <td style="text-align: center">Tier 1 leverage ratio</td>
      <td style="text-align: center">CAP</td>
      <td style="text-align: right">Positive</td>
    </tr>
    <tr>
      <td style="text-align: left">Interest Margin</td>
      <td style="text-align: center">Net interest margin</td>
      <td style="text-align: center">NIM</td>
      <td style="text-align: right">Negative</td>
    </tr>
    <tr>
      <td style="text-align: left">Asset Quality</td>
      <td style="text-align: center">Nonaccrual loans over total loans</td>
      <td style="text-align: center">IMLOAN</td>
      <td style="text-align: right">Negative</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<ul>

<li><p align="justify"><b>Bank Size (SIZE)</b>. The size of banks (SIZE) has been measured using the natural logarithm of total assets.  Sing et al. (2016) and El-Chaarani (2019) stated that bank size has a significant negative effect on liquidity.  Smaller banks rely on a buffer of liquid assets while larger banks rely on credit instruments and the inter-bank market which is in accordance with the “too big to fail” hypothesis.</p></li>
<li><p align="justify"><b>Profitability (ROA)</b>.  Return on assets is the proxy of profitability in this article.  Return on Assets (TTM) measures the net income over the prior four quarters over average total assets.  Moussa (2015) found that banks with higher return on assets had lower levels of liquidity.  Banks with less liquid assets have a higher return on assets due to liquid assets generally yielding less income than illiquid assets.</p></li>
<li><p align="justify"><b>Deposits (DEP)</b>.  Deposits (DEP) have been calculated as total deposits over total assets.  Sing et al. (2016) stated that the level of deposits has a positive effect on liquidity.  Deposits are the major source of funds for banks and these institutions must maintain adequate levels of liquidity to meet customer demand. </p></li>
<li><p align="justify"><b>Capital Adequacy (CAP)</b>.  The tier 1 leverage ratio is the proxy of capital adequacy in this article. Sing et al. (2016) and Alger et al. (1999) found that capital adequacy had a positive influence on liquidity as a whole.  Alger et al. (1999) suggests that banks invest more in liquid assets as a precaution when more capital is at stake. </p></li>
<li><p align="justify"><b>Interest Margin (NIM)</b>.  Net interest margin is the measure of interest income less interest expenses over average earning assets.  According to finance theory, interest margin should be negatively correlated with liquidity.  Vodová (2013) found that liquidity is negatively affected by interest margin.  An increase in interest margin incentivises banks to focus more on lending activities resulting in a decline in liquid assets. </p></li>
<li><p align="justify"><b>Asset Quality (IMLOAN)</b>.  Total nonaccrual loans over total loans is the proxy of asset quality in this article.  Munteanu (2012) stated that the level impaired loans has a significant effect on liquidity.  A higher level of nonaccrual loans may lead to banks increasing lending activity to make up for losses thus decreasing the level of liquid assets. </p></li>

</ul>

<p style="text-align: left;"><font size="+3"><b>Model Specification</b></font></p>

<p>Two models will be estimated:</p>

\[Liq1_{it} = \alpha_{it}+β_{1}SIZE_{it}+β_{2}ROA_{it}+β_{3}DEP_{it}+β_{4}CAP_{it}+β_{5}IMLOAN_{it}+ ε_{it}\]

\[Liq2_{it} = \alpha_{it}+β_{1}SIZE_{it}+β_{2}ROA_{it}+β_{3}DEP_{it}+β_{4}CAP_{it}+β_{5}IMLOAN_{it}+ ε_{it}\]

<p align="justify">
β1, β2 , β3, β4, and β5 are the coefficients of the determinant variables and ε is the error term.  The panel data was constructed with indices “i” and “t” representing individual banks and time, respectively.  The data is comprised of 59 banks from December 31st, 2019, until June 30th, 2023.  The total number of observations is 885.  
</p>

<p style="text-align: left;"><font size="+3"><b>Descriptive Statistics</b></font></p>

<p align="justify">
Table 1 highlights the descriptive statistics for state chartered Michigan banks.  Based on the findings below, the average net loans as a fraction of total assets (Liq1) is 59 percent for michigan banks.  Additionally, the average level of liquid assets as a fraction of total assets (Liq2) is 33 percent.  
</p>

<p>Table 1 
<br />
Descriptive Statistics (Sample 12/31/2019 - 0630/2023)</p>
<div class="img">
    <img class="col three" src="/al-folio/assets/img/summary_stats.png" />
</div>

<p><br /></p>

<p style="text-align: left;"><font size="+3"><b>Fixed Effect and Random Effect Regression</b></font></p>

<p align="justify">
Fixed effect and random effect regressions were run and a Hausman test was carried out to choose the appropriate model.  The Hausman Test for both measures of liquidity resulted in a p-value less than 0.05, confirming that the fixed effect regression is the preferred model for Liq1 and Liq2.  Results are shown in Table 2. 
<br />
<br />

The fixed effect models concluded that DEP, IMLOAN, CAP, NIM, and ROA significantly impacted the fist measure of liquidity (Liq1) as well as the second measure(Liq2).  The impact of DEP, IMLOAN, and ROA on Liq1 (net loans / total assets) was negative whereas the impact of CAP and NIM was positive.  However, the impact of DEP, IMLOAN, and ROA on Liq2 (liquid assets / total assets) was positive while the impact of CAP and NIM was negative.  SIZE was found to have an insignificant effect on either liquidity measure (Table 3).  
</p>

<p>Table2
<br />
Hausman Test</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Model</th>
      <th style="text-align: center">Chi-Sq. Statistic</th>
      <th style="text-align: right">Prob.</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Liq1 Model</td>
      <td style="text-align: center">33.965</td>
      <td style="text-align: right">6.833e-06</td>
    </tr>
    <tr>
      <td style="text-align: left">Liq2 Model</td>
      <td style="text-align: center">56.274</td>
      <td style="text-align: right">2.562e-10</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<p>Table 3
<br />
Regression Analysis</p>

<div class="img">
    <img class="col three" src="/al-folio/assets/img/regressionresults.png" />
</div>

<p style="text-align: left;"><font size="+3"><b>Conclusion</b></font></p>

<p style="text-align: left;"><font size="+3"><b>References</b></font></p>
<p align="justify">
Alger, G., &amp; Alger, I. (1999). Liquid assets in banks: Theory and practice. GREMAQ, Universite des Sciences Sociales.
</p>

<p align="justify">
El-Chaarani, H. (2019). Determinants of bank liquidity in the Middle East region. El-CHAARANI H.,(2019), Determinants of Bank Liquidity in the Middle East Region, International Review of Management and Marketing, 9(2).
</p>

<p align="justify">
Moussa, M. A. B. (2015). The determinants of bank liquidity: Case of Tunisia. International journal of economics and financial issues, 5(1), 249-259.
</p>

<p align="justify">
Munteanu, I. (2012). Bank liquidity and its determinants in Romania. Procedia Economics and Finance, 3, 993-998.
</p>

<p align="justify">
Singh, A., &amp; Sharma, A. K. (2016). An empirical analysis of macroeconomic and bank-specific factors affecting liquidity of Indian banks. Future Business Journal, 2(1), 40-53.
</p>

<p align="justify">
Vodová, P. (2011). Liquidity of Czech commercial banks and its determinants. International Journal of mathematical models and methods in applied sciences, 5(6), 1060-1067.
</p>

<p align="justify">
Vodová, P. (2013). Determinants of commercial bank liquidity in Hungary. Finansowy Kwartalnik Internetowy e-Finanse, 9(3), 64-71.
</p>]]></content><author><name></name></author><category term="Liquidity" /><category term="Banking" /><category term="Michigan" /><summary type="html"><![CDATA[Introduction The objective of this article is to examine bank specific factors that determine the level of liquid assets held by state chartered commercial banks in Michigan. Bank specific factors include bank size, profitability, capital adequacy, deposits, loans, and asset quality. This article also aims to explore ongoing liquidity trends impacting these financial institutions. This study consists of balanced panel data of 59 state chartered commercial banks from December 31st, 2019, until June 30th, 2023. Under the fractional reserve banking system, banks only hold a fraction of their deposit liabilities in liquid assets to meet anticipated liquidity needs. Bank management is responsible for determining the appropriate level of liquid assets required to meet the demands of depositors and creditors while allowing for the greatest profitability. Developing a better understanding of the determinants of liquid assets will give key insights into liquidity trends. Literature Review In 1999, an analysis of the Mexican banking system found that capital has a significant positive effect on liquid assets and the total asset size of the institutions (in relation to the total assets of the banking system) has no statistically significant effect on any measure of liquid assets. This study also theorized that banks with relatively more demand deposits have more liquid assets but found that the level of demand deposits relative to total assets has a significant negative effect on securities and liquid assets but not the level of cash. This finding could suggest that banks consider demand deposits to be a very stable liability, or the variance of deposit withdraws declines as deposits increase and the deposit population becomes more diverse (Alger &amp; Alger, 1999). The relationship between liquid assets and bank size was further examined in a 2015 study of 18 Tunisian banks. This study used the natural logarithm of total assets as a measure of bank size and also found that bank size does not have a significant impact on bank liquidity supporting the findings of. Additional variables in this study include return on assets (ROA), return on equity (ROE), net interest margin (NIM), total loans, operating expenses, total deposits, financial expenses, equity, inflation rate, and growth rate of GDP. ROA, ROE, capital, operating expenses, growth rate of GDP, and inflation rate were found to have a significant impact on bank liquidity. Total loans, financial costs, and total deposits did not have a significant impact on bank liquidity (Moussa, 2015). Macroeconomic as well as bank specific determinants of liquidity of Indian banks were studied by Singh and Sharma (2016). Bank specific determinants studied in this paper include bank size, profitability, cost of funding, capital adequacy, and deposits. Macroeconomic determinants of liquidity considered include GDP, inflation, and unemployment. This study found that bank size and GDP have a negative relationship with liquidity at a 5 percent confidence interval while profitability, deposits, inflation, and capital adequacy have a positive impact. Specification of Variables The dependent variable in this article will be liquidity which will be measured through two liquidity ratios provided by Vodov’a (2011). Fixed and random effect regression will be performed for both measures of liquidity. The first measure of liquidity is net loans as a fraction of total assets (Liq1). The ratio Liq1 indicates the percentage of bank assets which are invested in illiquid loans. The higher this ratio is, the fewer liquid assets a bank possesses. The second measure of liquidity is liquid assets as a fraction of total assets (Liq2). The ratio Liq2 indicates the percentage of bank assets which are liquid. A high ratio may indicate an inefficiency since liquid assets yield less income than illiquid assets. Liquid assets consist of interest as well as non-interest bearing bank balances, securities sold under agreement to repurchase, federal funds sold, and total securities less pledged securities. Independent Variable Proxy/Measurement Notation Expected Effect Bank Size Natural log of total assets SIZE Negative Profitability Return on assets ROA Negative Deposits Deposits over total assets DEP Positive Capital Adequacy Tier 1 leverage ratio CAP Positive Interest Margin Net interest margin NIM Negative Asset Quality Nonaccrual loans over total loans IMLOAN Negative Bank Size (SIZE). The size of banks (SIZE) has been measured using the natural logarithm of total assets. Sing et al. (2016) and El-Chaarani (2019) stated that bank size has a significant negative effect on liquidity. Smaller banks rely on a buffer of liquid assets while larger banks rely on credit instruments and the inter-bank market which is in accordance with the “too big to fail” hypothesis. Profitability (ROA). Return on assets is the proxy of profitability in this article. Return on Assets (TTM) measures the net income over the prior four quarters over average total assets. Moussa (2015) found that banks with higher return on assets had lower levels of liquidity. Banks with less liquid assets have a higher return on assets due to liquid assets generally yielding less income than illiquid assets. Deposits (DEP). Deposits (DEP) have been calculated as total deposits over total assets. Sing et al. (2016) stated that the level of deposits has a positive effect on liquidity. Deposits are the major source of funds for banks and these institutions must maintain adequate levels of liquidity to meet customer demand. Capital Adequacy (CAP). The tier 1 leverage ratio is the proxy of capital adequacy in this article. Sing et al. (2016) and Alger et al. (1999) found that capital adequacy had a positive influence on liquidity as a whole. Alger et al. (1999) suggests that banks invest more in liquid assets as a precaution when more capital is at stake. Interest Margin (NIM). Net interest margin is the measure of interest income less interest expenses over average earning assets. According to finance theory, interest margin should be negatively correlated with liquidity. Vodová (2013) found that liquidity is negatively affected by interest margin. An increase in interest margin incentivises banks to focus more on lending activities resulting in a decline in liquid assets. Asset Quality (IMLOAN). Total nonaccrual loans over total loans is the proxy of asset quality in this article. Munteanu (2012) stated that the level impaired loans has a significant effect on liquidity. A higher level of nonaccrual loans may lead to banks increasing lending activity to make up for losses thus decreasing the level of liquid assets. Model Specification Two models will be estimated: \[Liq1_{it} = \alpha_{it}+β_{1}SIZE_{it}+β_{2}ROA_{it}+β_{3}DEP_{it}+β_{4}CAP_{it}+β_{5}IMLOAN_{it}+ ε_{it}\] \[Liq2_{it} = \alpha_{it}+β_{1}SIZE_{it}+β_{2}ROA_{it}+β_{3}DEP_{it}+β_{4}CAP_{it}+β_{5}IMLOAN_{it}+ ε_{it}\] β1, β2 , β3, β4, and β5 are the coefficients of the determinant variables and ε is the error term. The panel data was constructed with indices “i” and “t” representing individual banks and time, respectively. The data is comprised of 59 banks from December 31st, 2019, until June 30th, 2023. The total number of observations is 885. Descriptive Statistics Table 1 highlights the descriptive statistics for state chartered Michigan banks. Based on the findings below, the average net loans as a fraction of total assets (Liq1) is 59 percent for michigan banks. Additionally, the average level of liquid assets as a fraction of total assets (Liq2) is 33 percent. Table 1 Descriptive Statistics (Sample 12/31/2019 - 0630/2023) Fixed Effect and Random Effect Regression Fixed effect and random effect regressions were run and a Hausman test was carried out to choose the appropriate model. The Hausman Test for both measures of liquidity resulted in a p-value less than 0.05, confirming that the fixed effect regression is the preferred model for Liq1 and Liq2. Results are shown in Table 2. The fixed effect models concluded that DEP, IMLOAN, CAP, NIM, and ROA significantly impacted the fist measure of liquidity (Liq1) as well as the second measure(Liq2). The impact of DEP, IMLOAN, and ROA on Liq1 (net loans / total assets) was negative whereas the impact of CAP and NIM was positive. However, the impact of DEP, IMLOAN, and ROA on Liq2 (liquid assets / total assets) was positive while the impact of CAP and NIM was negative. SIZE was found to have an insignificant effect on either liquidity measure (Table 3). Table2 Hausman Test Model Chi-Sq. Statistic Prob. Liq1 Model 33.965 6.833e-06 Liq2 Model 56.274 2.562e-10 Table 3 Regression Analysis Conclusion References Alger, G., &amp; Alger, I. (1999). Liquid assets in banks: Theory and practice. GREMAQ, Universite des Sciences Sociales. El-Chaarani, H. (2019). Determinants of bank liquidity in the Middle East region. El-CHAARANI H.,(2019), Determinants of Bank Liquidity in the Middle East Region, International Review of Management and Marketing, 9(2). Moussa, M. A. B. (2015). The determinants of bank liquidity: Case of Tunisia. International journal of economics and financial issues, 5(1), 249-259. Munteanu, I. (2012). Bank liquidity and its determinants in Romania. Procedia Economics and Finance, 3, 993-998. Singh, A., &amp; Sharma, A. K. (2016). An empirical analysis of macroeconomic and bank-specific factors affecting liquidity of Indian banks. Future Business Journal, 2(1), 40-53. Vodová, P. (2011). Liquidity of Czech commercial banks and its determinants. International Journal of mathematical models and methods in applied sciences, 5(6), 1060-1067. Vodová, P. (2013). Determinants of commercial bank liquidity in Hungary. Finansowy Kwartalnik Internetowy e-Finanse, 9(3), 64-71.]]></summary></entry><entry><title type="html">Exploring scikit-learn</title><link href="https://danieljunghans.github.io/al-folio/blog/2020/SCIKITLearn/" rel="alternate" type="text/html" title="Exploring scikit-learn" /><published>2020-08-28T02:34:00+00:00</published><updated>2020-08-28T02:34:00+00:00</updated><id>https://danieljunghans.github.io/al-folio/blog/2020/SCIKITLearn</id><content type="html" xml:base="https://danieljunghans.github.io/al-folio/blog/2020/SCIKITLearn/"><![CDATA[<p style="text-align: center;"><font size="+3">Introduction</font></p>
<p>For my current NEAT project, I read the paper <a href="https://ieeexplore.ieee.org/abstract/document/8473214?casa_token=mA1Va18Dm6kAAAAA:v_6_aQSag5JUXPvV3uPm-BYIVUfWLtCD5HZFDXopj5UUDriA460pLKGfCr99nKgQEYCw8a-GAQ"><em>A Comparative Study of Supervised Machine Learning Algorithms for Stock Market Trend Prediction</em></a> to learn  how others approached stock market prediction using supervised machine learning algorithms. This paper compared the accuracy between Support Vector Machine, Random Forest, K-Nearest Neighbor, Naive Bayes, and SoftMax algorithms. After reading about these different algorithms, I took it upon myself to learn more about the scikit-learn python library. I am learning how to use scikit-learn because it will give me the ability to implement some the algorithms outlined in the paper. The algorithms I have covered so far include: <br />
<a href="#RandomForest">•	Random Forest</a><br />
<a href="#KNN">•	K-Nearest Neighbors</a><br />
<a href="#SVM">•	Support Vector Machine</a><br /></p>

<p><a name="RandomForest"></a></p>
<p style="text-align: center;"><font size="+3">Random Forest</font></p>
<p>The first algorithm I sought to better understand was the Random Forest algorithm. This algorithm is made up of multiple decision trees. A decision tree learns simple decision rules from the data and produces a output. Each tree is used on a different sub sample of the dataset and averaging is used to improve accuracy. The decision trees all “vote” by producing outputs, and whatever the most popular output is becomes the algorithms output.</p>

<p>I first imported my data as a Pandas data frame and split it up into a training section and a testing section. I then created a Random Forest algorithm using the scikit-learn random forest classifier. After running the algorithm a few times, I noticed that the accuracy was unusually high. After some investigation, I discovered that the algorithms were predicting the stock price movement for the current day instead of one day in the future. After fixing the problem, the accuracy on the testing dataset stayed around 50%. Here is the code that I wrote:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="n">csv</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="n">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="n">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>

<span class="c1">#this opens the file and puts the data in a pandas dataframe
</span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">RandomForest.csv</span><span class="sh">'</span><span class="p">)</span>
<span class="n">data</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>

<span class="c1">#setting the size of the training dataset
</span><span class="n">training_size</span> <span class="o">=</span> <span class="p">.</span><span class="mi">7</span>
<span class="n">split</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">*</span><span class="p">.</span><span class="mi">7</span><span class="p">)</span>

<span class="c1">#Identifying all of the inputs and the expected output
</span><span class="n">X</span><span class="o">=</span><span class="n">data</span><span class="p">[[</span><span class="sh">'</span><span class="s">Open</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">High</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Low</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Close</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Volume</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Accumulation Distribution Line</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">MACD</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Chaikan Oscillator (CHO)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Highest closing price (5 days)</span><span class="sh">'</span><span class="p">,</span>
<span class="sh">'</span><span class="s">Lowest closing price (days)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Stochastic %K (5 days)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">%D</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Volume Price Trend (VPT)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Williams %R (14 days)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Relative Strength Index</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Momentum (10 days)</span><span class="sh">'</span><span class="p">,</span>
<span class="sh">'</span><span class="s">Price rate of change (PROC)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Volume rate of change (VROC)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">On Balance Volume (OBV)</span><span class="sh">'</span><span class="p">]]</span>
<span class="n">y</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">Outputs</span><span class="sh">'</span><span class="p">]</span>

<span class="c1">#splitting the dataframe into a training and testing dataset
</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="n">split</span><span class="p">]</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">split</span><span class="p">:]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="n">split</span><span class="p">]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">split</span><span class="p">:]</span>

<span class="c1">#create a the random forest with 500 decision trees
</span><span class="n">clf</span><span class="o">=</span><span class="nc">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span> 

<span class="c1">#train the model on the training data
</span><span class="n">clf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="c1">#making predictions on the testing data
</span><span class="n">y_pred</span><span class="o">=</span><span class="n">clf</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1">#printing the testing accuracy
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Accuracy:</span><span class="sh">"</span><span class="p">,</span><span class="n">metrics</span><span class="p">.</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p><a name="KNN"></a></p>
<p style="text-align: center;"><font size="+3">K-Nearest Neighbors</font></p>
<p>The next algorithm I studied was the K-Nearest Neighbors algorithm. The nearest neighbor method looks for the most similar trading days. The predefined number of the most similar trading days become the nearest neighbors. Just like the decision trees in the Random Forest algorithm, the nearest neighbors “vote”. Instead of using decision trees to produce an output, the K-Nearest Neighbor algorithm uses the actual expected output for the most similar trading days. The most popular output becomes of the neighbors becomes the algorithms output. If 5 of the nearest neighbors had the closing stock price go up and 3 neighbors had the stock price go down, the prediction will be that the stock price goes up.</p>

<p>While studying this algorithm, I learned that K-Nearest Neighbor algorithms performs better with a lower number of features/inputs. To reduce the size of my dataset, I first normalized it with the standard scaler tool from scikit-learn. I then used PCA to shrink the number of inputs in my dataset down to two while preserving the variance in the dataset.  Here is my K-Nearest Neighbors code:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="n">csv</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>
<span class="kn">from</span> <span class="n">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="n">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1">#this opens the CSV File
</span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">RandomForest.csv</span><span class="sh">'</span><span class="p">)</span>
<span class="n">data</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>

<span class="c1">#preprocessing and normalizing
</span><span class="n">Features</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">Open</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">High</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Low</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Close</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Volume</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Accumulation Distribution Line</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">MACD</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Chaikan Oscillator (CHO)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Highest closing price (5 days)</span><span class="sh">'</span><span class="p">,</span>
<span class="sh">'</span><span class="s">Lowest closing price (days)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Stochastic %K (5 days)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">%D</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Volume Price Trend (VPT)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Williams %R (14 days)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Relative Strength Index</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Momentum (10 days)</span><span class="sh">'</span><span class="p">,</span>
<span class="sh">'</span><span class="s">Price rate of change (PROC)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Volume rate of change (VROC)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">On Balance Volume (OBV)</span><span class="sh">'</span><span class="p">]</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">Features</span><span class="p">].</span><span class="n">values</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="sh">'</span><span class="s">Outputs</span><span class="sh">'</span><span class="p">]].</span><span class="n">values</span>

<span class="c1">#normalizing the dataset
</span><span class="n">X</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">().</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1">#performing PCA
</span><span class="n">PCA</span> <span class="o">=</span> <span class="nc">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">Components</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">ComponentDf</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">Components</span><span class="p">)</span>

<span class="c1">#setting the size of the training set
</span><span class="n">training_size</span> <span class="o">=</span> <span class="p">.</span><span class="mi">8</span>
<span class="n">split</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">*</span><span class="n">training_size</span><span class="p">)</span>

<span class="c1">#splitting up the dataset
</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">ComponentDf</span><span class="p">[:</span><span class="n">split</span><span class="p">])</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">ComponentDf</span><span class="p">[</span><span class="n">split</span><span class="p">:])</span>
<span class="n">Y_train</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">Y</span><span class="p">[:</span><span class="n">split</span><span class="p">])</span>
<span class="n">Y_test</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">Y</span><span class="p">[</span><span class="n">split</span><span class="p">:])</span>

<span class="c1">#create the K Nearest Neighbor Algorithm and running the algorithm
</span><span class="n">clf</span><span class="o">=</span><span class="nc">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">clf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="nf">ravel</span><span class="p">())</span>
<span class="n">Y_pred</span><span class="o">=</span><span class="n">clf</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1">#printing the testing accuracy 
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Accuracy:</span><span class="sh">"</span><span class="p">,</span><span class="n">metrics</span><span class="p">.</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>After performing principal component analysis on my dataset, I graphed the principal components to better understand the data. After creating the graph, it became clear why the accuracy of my K-Nearest Neighbor algorithm stayed around 50%. Both expected outputs (Stock price going up or down) are clustered together.</p>

<div class="img">
    <img class="col three" src="/al-folio/assets/img/Component.PNG" />
</div>

<p><a name="SVM"></a></p>
<p style="text-align: center;"><font size="+3">Support Vector Machine</font></p>
<p>A Support Vector Machine is a machine learning tool that uses a hyperplane to define data points. Instead of looking at the nearest neighbors like a KNN, I like to think that Support Vector Machines split up data points into different neighborhoods. For example, if we use the two components from my PCA analysis, a SVM will create a one dimensional hyperplane splitting up the data into two “neighborhoods”. The hyperplane will create decision boundaries that maximize the margins from both expected outputs. The graph below shows the data points and the decision boundaries created by my SVM.</p>

<div class="img">
    <img class="col three" src="/al-folio/assets/img/graph10.png" />
</div>

<p>Any data point that falls into the blue background gets categorized as a 0 (closing stock price goes down). To create this graph, I used a Support Vector Machine with a RBF kernel. Kernel functions allow SVMs to create hyperplanes in high dimensional data without having to calculate the coordinates of the data in that space. Here is my Support Vector Machine Code:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
</pre></td><td class="code"><pre><span class="c1">#support vector machine
</span>
<span class="kn">import</span> <span class="n">csv</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>
<span class="kn">from</span> <span class="n">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="kn">from</span> <span class="n">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1">#this opens the CSV File
</span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">RandomForest.csv</span><span class="sh">'</span><span class="p">)</span>
<span class="n">data</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>

<span class="c1">#setting the size of the training dataset
</span><span class="n">training_size</span> <span class="o">=</span> <span class="p">.</span><span class="mi">7</span>
<span class="n">split</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">*</span><span class="p">.</span><span class="mi">7</span><span class="p">)</span>

<span class="c1">#Identifying all of the inputs and the expected output
</span><span class="n">X</span><span class="o">=</span><span class="n">data</span><span class="p">[[</span><span class="sh">'</span><span class="s">Open</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">High</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Low</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Close</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Volume</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Accumulation Distribution Line</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">MACD</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Chaikan Oscillator (CHO)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Highest closing price (5 days)</span><span class="sh">'</span><span class="p">,</span>
<span class="sh">'</span><span class="s">Lowest closing price (days)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Stochastic %K (5 days)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">%D</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Volume Price Trend (VPT)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Williams %R (14 days)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Relative Strength Index</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Momentum (10 days)</span><span class="sh">'</span><span class="p">,</span>
<span class="sh">'</span><span class="s">Price rate of change (PROC)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Volume rate of change (VROC)</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">On Balance Volume (OBV)</span><span class="sh">'</span><span class="p">]]</span>
<span class="n">y</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">Outputs</span><span class="sh">'</span><span class="p">]</span>

<span class="c1">#splitting the dataframe into a training and testing dataset
</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="n">split</span><span class="p">]</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">split</span><span class="p">:]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="n">split</span><span class="p">]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">split</span><span class="p">:]</span>

<span class="c1">#create the support vector machine
</span><span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="p">.</span><span class="nc">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="sh">'</span><span class="s">rbf</span><span class="sh">'</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">svc</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">svc</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1">#printing the testing accuracy
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Accuracy:</span><span class="sh">"</span><span class="p">,</span><span class="n">metrics</span><span class="p">.</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>

<span class="c1">##################################################
#The next section transforms the dataset#
#And graphs the components with decision boundries
</span>
<span class="c1">#Normalizing the dataset
</span><span class="n">X</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">().</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1">#performing PCA
</span><span class="n">PCA</span> <span class="o">=</span> <span class="nc">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">Components</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">ComponentDf</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">Components</span><span class="p">,</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">principal component 1</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">principal component 2</span><span class="sh">'</span><span class="p">])</span>

<span class="c1">#transforming the dataset
</span><span class="n">X</span> <span class="o">=</span> <span class="n">ComponentDf</span><span class="p">.</span><span class="nf">to_numpy</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="nf">ravel</span><span class="p">()</span>

<span class="n">h</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="c1"># create a mesh to plot in
</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">].</span><span class="nf">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">].</span><span class="nf">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">].</span><span class="nf">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">].</span><span class="nf">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span>
                     <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span>

<span class="c1">#create the support vector machine
</span><span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="p">.</span><span class="nc">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="sh">'</span><span class="s">rbf</span><span class="sh">'</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">).</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1">#graphing the data
</span><span class="n">Z</span> <span class="o">=</span> <span class="n">svc</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="p">.</span><span class="nf">ravel</span><span class="p">()])</span>

<span class="c1"># Put the result into a color plot
</span><span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">xx</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="p">.</span><span class="n">cm</span><span class="p">.</span><span class="n">coolwarm</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

<span class="c1"># Creating the plot
</span><span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="p">.</span><span class="n">cm</span><span class="p">.</span><span class="n">coolwarm</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Principal Component 1</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Principal Component 2</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">(</span><span class="n">xx</span><span class="p">.</span><span class="nf">min</span><span class="p">(),</span> <span class="n">xx</span><span class="p">.</span><span class="nf">max</span><span class="p">())</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="n">yy</span><span class="p">.</span><span class="nf">min</span><span class="p">(),</span> <span class="n">yy</span><span class="p">.</span><span class="nf">max</span><span class="p">())</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xticks</span><span class="p">(())</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">yticks</span><span class="p">(())</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Support Vector Machine Graph</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="c1"># Saving the Plot
</span><span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">"</span><span class="s">matplotlib.png</span><span class="sh">"</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>]]></content><author><name></name></author><category term="Machine-Learning" /><summary type="html"><![CDATA[Introduction For my current NEAT project, I read the paper A Comparative Study of Supervised Machine Learning Algorithms for Stock Market Trend Prediction to learn how others approached stock market prediction using supervised machine learning algorithms. This paper compared the accuracy between Support Vector Machine, Random Forest, K-Nearest Neighbor, Naive Bayes, and SoftMax algorithms. After reading about these different algorithms, I took it upon myself to learn more about the scikit-learn python library. I am learning how to use scikit-learn because it will give me the ability to implement some the algorithms outlined in the paper. The algorithms I have covered so far include: • Random Forest • K-Nearest Neighbors • Support Vector Machine Random Forest The first algorithm I sought to better understand was the Random Forest algorithm. This algorithm is made up of multiple decision trees. A decision tree learns simple decision rules from the data and produces a output. Each tree is used on a different sub sample of the dataset and averaging is used to improve accuracy. The decision trees all “vote” by producing outputs, and whatever the most popular output is becomes the algorithms output. I first imported my data as a Pandas data frame and split it up into a training section and a testing section. I then created a Random Forest algorithm using the scikit-learn random forest classifier. After running the algorithm a few times, I noticed that the accuracy was unusually high. After some investigation, I discovered that the algorithms were predicting the stock price movement for the current day instead of one day in the future. After fixing the problem, the accuracy on the testing dataset stayed around 50%. Here is the code that I wrote: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import csv import pandas as pd from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn import metrics from sklearn.metrics import classification_report #this opens the file and puts the data in a pandas dataframe data = pd.read_csv('RandomForest.csv') data.head() #setting the size of the training dataset training_size = .7 split = int(len(data)*.7) #Identifying all of the inputs and the expected output X=data[['Open','High','Low','Close','Volume','Accumulation Distribution Line','MACD','Chaikan Oscillator (CHO)','Highest closing price (5 days)', 'Lowest closing price (days)','Stochastic %K (5 days)','%D','Volume Price Trend (VPT)','Williams %R (14 days)','Relative Strength Index','Momentum (10 days)', 'Price rate of change (PROC)','Volume rate of change (VROC)','On Balance Volume (OBV)']] y=data['Outputs'] #splitting the dataframe into a training and testing dataset X_train = X[:split] X_test = X[split:] y_train = y[:split] y_test = y[split:] #create a the random forest with 500 decision trees clf=RandomForestClassifier(n_estimators=500) #train the model on the training data clf.fit(X_train,y_train) #making predictions on the testing data y_pred=clf.predict(X_test) #printing the testing accuracy print("Accuracy:",metrics.accuracy_score(y_test, y_pred)) K-Nearest Neighbors The next algorithm I studied was the K-Nearest Neighbors algorithm. The nearest neighbor method looks for the most similar trading days. The predefined number of the most similar trading days become the nearest neighbors. Just like the decision trees in the Random Forest algorithm, the nearest neighbors “vote”. Instead of using decision trees to produce an output, the K-Nearest Neighbor algorithm uses the actual expected output for the most similar trading days. The most popular output becomes of the neighbors becomes the algorithms output. If 5 of the nearest neighbors had the closing stock price go up and 3 neighbors had the stock price go down, the prediction will be that the stock price goes up. While studying this algorithm, I learned that K-Nearest Neighbor algorithms performs better with a lower number of features/inputs. To reduce the size of my dataset, I first normalized it with the standard scaler tool from scikit-learn. I then used PCA to shrink the number of inputs in my dataset down to two while preserving the variance in the dataset. Here is my K-Nearest Neighbors code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import csv import pandas as pd from sklearn import metrics from sklearn.metrics import classification_report from sklearn.neighbors import KNeighborsClassifier from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler #this opens the CSV File data = pd.read_csv('RandomForest.csv') data.head() #preprocessing and normalizing Features = ['Open','High','Low','Close','Volume','Accumulation Distribution Line','MACD','Chaikan Oscillator (CHO)','Highest closing price (5 days)', 'Lowest closing price (days)','Stochastic %K (5 days)','%D','Volume Price Trend (VPT)','Williams %R (14 days)','Relative Strength Index','Momentum (10 days)', 'Price rate of change (PROC)','Volume rate of change (VROC)','On Balance Volume (OBV)'] X = data.loc[:, Features].values Y = data.loc[:,['Outputs']].values #normalizing the dataset X = StandardScaler().fit_transform(X) #performing PCA PCA = PCA(n_components=2) Components = PCA.fit_transform(X) ComponentDf = pd.DataFrame(data=Components) #setting the size of the training set training_size = .8 split = int(len(data)*training_size) #splitting up the dataset X_train = pd.DataFrame(data=ComponentDf[:split]) X_test = pd.DataFrame(data=ComponentDf[split:]) Y_train = pd.DataFrame(data=Y[:split]) Y_test = pd.DataFrame(data=Y[split:]) #create the K Nearest Neighbor Algorithm and running the algorithm clf=KNeighborsClassifier(n_neighbors=5) clf.fit(X_train, Y_train.values.ravel()) Y_pred=clf.predict(X_test) #printing the testing accuracy print("Accuracy:",metrics.accuracy_score(Y_test, Y_pred)) After performing principal component analysis on my dataset, I graphed the principal components to better understand the data. After creating the graph, it became clear why the accuracy of my K-Nearest Neighbor algorithm stayed around 50%. Both expected outputs (Stock price going up or down) are clustered together. Support Vector Machine A Support Vector Machine is a machine learning tool that uses a hyperplane to define data points. Instead of looking at the nearest neighbors like a KNN, I like to think that Support Vector Machines split up data points into different neighborhoods. For example, if we use the two components from my PCA analysis, a SVM will create a one dimensional hyperplane splitting up the data into two “neighborhoods”. The hyperplane will create decision boundaries that maximize the margins from both expected outputs. The graph below shows the data points and the decision boundaries created by my SVM. Any data point that falls into the blue background gets categorized as a 0 (closing stock price goes down). To create this graph, I used a Support Vector Machine with a RBF kernel. Kernel functions allow SVMs to create hyperplanes in high dimensional data without having to calculate the coordinates of the data in that space. Here is my Support Vector Machine Code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 #support vector machine import csv import pandas as pd from sklearn import metrics from sklearn.metrics import classification_report from sklearn import svm from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler import matplotlib.pyplot as plt import numpy as np #this opens the CSV File data = pd.read_csv('RandomForest.csv') data.head() #setting the size of the training dataset training_size = .7 split = int(len(data)*.7) #Identifying all of the inputs and the expected output X=data[['Open','High','Low','Close','Volume','Accumulation Distribution Line','MACD','Chaikan Oscillator (CHO)','Highest closing price (5 days)', 'Lowest closing price (days)','Stochastic %K (5 days)','%D','Volume Price Trend (VPT)','Williams %R (14 days)','Relative Strength Index','Momentum (10 days)', 'Price rate of change (PROC)','Volume rate of change (VROC)','On Balance Volume (OBV)']] y=data['Outputs'] #splitting the dataframe into a training and testing dataset X_train = X[:split] X_test = X[split:] y_train = y[:split] y_test = y[split:] #create the support vector machine svc = svm.SVC(kernel='rbf', C=1.0) svc.fit(X_train, y_train) y_pred = svc.predict(X_test) #printing the testing accuracy print("Accuracy:",metrics.accuracy_score(y_test, y_pred)) ################################################## #The next section transforms the dataset# #And graphs the components with decision boundries #Normalizing the dataset X = StandardScaler().fit_transform(X) #performing PCA PCA = PCA(n_components=2) Components = PCA.fit_transform(X) ComponentDf = pd.DataFrame(data=Components,columns = ['principal component 1', 'principal component 2']) #transforming the dataset X = ComponentDf.to_numpy() y = y.ravel() h = 0.2 # create a mesh to plot in x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) #create the support vector machine svc = svm.SVC(kernel='rbf', C=1.0).fit(X, y) #graphing the data Z = svc.predict(np.c_[xx.ravel(), yy.ravel()]) # Put the result into a color plot Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8) # Creating the plot plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm) plt.xlabel('Principal Component 1') plt.ylabel('Principal Component 2') plt.xlim(xx.min(), xx.max()) plt.ylim(yy.min(), yy.max()) plt.xticks(()) plt.yticks(()) plt.title('Support Vector Machine Graph') plt.legend() # Saving the Plot plt.savefig("matplotlib.png")]]></summary></entry><entry><title type="html">2020 Summer Reflection</title><link href="https://danieljunghans.github.io/al-folio/blog/2020/Reflection/" rel="alternate" type="text/html" title="2020 Summer Reflection" /><published>2020-08-20T01:44:00+00:00</published><updated>2020-08-20T01:44:00+00:00</updated><id>https://danieljunghans.github.io/al-folio/blog/2020/Reflection</id><content type="html" xml:base="https://danieljunghans.github.io/al-folio/blog/2020/Reflection/"><![CDATA[<p style="text-align: center;"><font size="+3">Reflection</font></p>
<p>Looking back, I am very satisfied with the work I have done this summer. I started off the summer by working on my NEAT project and taking the course Financial Management 311. The beginning of this course focused on how to construct an income statement, balance sheet, and statement of cash flows. The next big concept this course covered was the time value of money and compound interest. I learned about the time value of money by doing present and future value calculations in excel. The last section of this course focused on bonds and capital budgeting decisions. I first learned about bonds and bond ratings, but quickly began performing NPV, payback period, discounted payback period, and IRR calculations.</p>

<p><br />
Financial Management 311 was very informative and a fun summer class. This course greatly expanded my knowledge of financial instruments and capital budgeting decisions. Although this class consumed a lot of my time, I was still working on other things. I continued my NEAT project during this time and was able to finish my website.</p>

<p><br /> 
After finishing Financial Management 311, I started taking Principles of Management Accounting 202. This course started by helping me understand the differences between financial and management accounting. Next, I began to learn about different cost classifications, and the basic manufacturing cost categories. With this new knowledge of cost classifications, I started to create traditional and contribution format income statements. Moving on from contribution income statements, I learned how to calculate the predetermined overhead rate and the different ways overhead is allocated. After learning about manufacturing overhead, I began to learn about costing methods and the flow of costs from raw materials to the cost of goods manufactured. Finally, the last few chapters for this course focused on Cost Volume Profit analysis, break even analysis, ROI, and Residual Income calculations.</p>

<p style="text-align: center;"><font size="+3">Summer Research Update</font></p>
<p>Throughout the summer, I continued to work on my NEAT project while taking the summer courses described above. After reading several papers about algorithms predicting stock price movement, I decided to change my algorithm so it is predicting whether the closing stock price will go up or down. Previously my algorithm predicting the closing price for the next day, but now it just predicts whether the price is going up or down. After changing the expected outputs, here was the algorithm accuracy for all models:</p>

<div class="img">
    <img class="col three" src="/al-folio/assets/img/graph9.PNG" />
</div>

<p>After running NEAT I created a random forrest algorithm using the python library <a href="https://scikit-learn.org/stable/">scikit learn</a>. The new random forrest algorithm performed very similiarly to my NEAT. For the next few weeks, I plan on becoming more familiar with the scikit learn library.</p>]]></content><author><name></name></author><category term="Updates" /><summary type="html"><![CDATA[Reflection Looking back, I am very satisfied with the work I have done this summer. I started off the summer by working on my NEAT project and taking the course Financial Management 311. The beginning of this course focused on how to construct an income statement, balance sheet, and statement of cash flows. The next big concept this course covered was the time value of money and compound interest. I learned about the time value of money by doing present and future value calculations in excel. The last section of this course focused on bonds and capital budgeting decisions. I first learned about bonds and bond ratings, but quickly began performing NPV, payback period, discounted payback period, and IRR calculations. Financial Management 311 was very informative and a fun summer class. This course greatly expanded my knowledge of financial instruments and capital budgeting decisions. Although this class consumed a lot of my time, I was still working on other things. I continued my NEAT project during this time and was able to finish my website. After finishing Financial Management 311, I started taking Principles of Management Accounting 202. This course started by helping me understand the differences between financial and management accounting. Next, I began to learn about different cost classifications, and the basic manufacturing cost categories. With this new knowledge of cost classifications, I started to create traditional and contribution format income statements. Moving on from contribution income statements, I learned how to calculate the predetermined overhead rate and the different ways overhead is allocated. After learning about manufacturing overhead, I began to learn about costing methods and the flow of costs from raw materials to the cost of goods manufactured. Finally, the last few chapters for this course focused on Cost Volume Profit analysis, break even analysis, ROI, and Residual Income calculations. Summer Research Update Throughout the summer, I continued to work on my NEAT project while taking the summer courses described above. After reading several papers about algorithms predicting stock price movement, I decided to change my algorithm so it is predicting whether the closing stock price will go up or down. Previously my algorithm predicting the closing price for the next day, but now it just predicts whether the price is going up or down. After changing the expected outputs, here was the algorithm accuracy for all models: After running NEAT I created a random forrest algorithm using the python library scikit learn. The new random forrest algorithm performed very similiarly to my NEAT. For the next few weeks, I plan on becoming more familiar with the scikit learn library.]]></summary></entry><entry><title type="html">NEAT Project</title><link href="https://danieljunghans.github.io/al-folio/blog/2020/NEAT/" rel="alternate" type="text/html" title="NEAT Project" /><published>2020-06-18T06:13:00+00:00</published><updated>2020-06-18T06:13:00+00:00</updated><id>https://danieljunghans.github.io/al-folio/blog/2020/NEAT</id><content type="html" xml:base="https://danieljunghans.github.io/al-folio/blog/2020/NEAT/"><![CDATA[<p style="text-align: center;"><font size="+3">Introduction</font></p>
<p>I began my current research project with a simple question: can I evolve an artificial neural network (ANN) to accurately predict the closing price of a stock?</p>

<p style="text-align: center;"><font size="+3">Artificial Neural Networks</font></p>
<p>ANNs are inspired by biological brains and are made up of densely interconnected nodes. Instead of neurons and synapses, ANNs utilize artificial neurons (nodes) and connections.</p>

<div class="img">
    <img class="col three" src="/al-folio/assets/img/neuralnetwork.png" />
</div>

<p style="text-align: center;"><i><strong>Figure 1</strong> Visual representation of an artificial neural network. The circles represent nodes and the lines connecting the nodes are connections. Nodes change their outputs based on the values received from connections.  </i></p>

<p><br /></p>

<h4>Input Layer</h4>
<p>All ANNs are made up of 3 types of layers. The first layer is the input layer shown in Figure 1. This is the initial data that is read by the neural networks. For this project, the input layer will consist of historical trading data. To learn about the inputs I am using, scroll down to the <a href="#stock">Stock Data section</a>.</p>

<h4>Connections</h4>
<p>Connections transmit signals in the form of real numbers and each connection has its own weight. The purpose of connection weights is to either strengthen or weaken the signal that is being transmitted. ANNs learn by adjusting their connection weights. As ANNs learn, important connections will develop larger weights as weights for unimportant connections become smaller.</p>

<h4>Hidden Layers</h4>
<p>The hidden layers come after the input layer and is where all the computation is done. The artificial neural network in Figure 1 has two hidden layers made up of four nodes in each layer. The nodes that are in the hidden layers must find the sum of all input signals multiplied by their corresponding weights, add a node bias, and plug the result in an Activation Function.</p>

<div class="img">
    <img class="col three" src="/al-folio/assets/img/node.png" />
</div>

<p style="text-align: center;"><i><strong>Figure 2</strong> visual representation of a node. The large circle represents the node and shows the addition of the bias to the summation of the inputs multiplied by the connection weights. The value the node calculates is then plugged into a non-linear activation function and the result is the node output. </i></p>

<p><br />
An Activation Function is a non-linear function used by nodes to produce an output. A common example of an Activation Function would be the sine function. The node bias shown in Figure 2 is a value added to the sum of all input signals multiplied by their weights. The purpose of adding a node bias is to move the Activation Function. The bias acts very similarly to the constant b in the linear function: <em>y=ax+b</em>. When the constant <em>b</em> is changed, the line will move up or down changing the <em>y-axis</em>. Moving an Activation Function greatly impacts the output of a node which may be useful in the learning process.</p>

<h4>Output Layer</h4>
<p>The last layer for Figure 1 is the output layer. The output layer is that last set of nodes that produce outputs. My ANNs will only have one node in the output layer. This last node will be responsible for outputting the closing price prediction.</p>

<p style="text-align: center;"><font size="+3">NEAT</font></p>
<p>While researching ANNs, I was introduced to the <a href="#references">Neural Evolution of Augmenting Topologies</a> (NEAT). NEAT was developed by Dr. Kenneth O. Stanely and is a method for evolving the structure ANNs. Unlike traditional ANNs that only adjust connection weights, NEAT gives ANNS the ability to change nodes, weights, and connections. The NEAT approach allows ANNS to add or delete connections and nodes. Stock market data is extremely noisy and allowing the structure of my ANNs to change may give them a competitive advantage.</p>

<p><a name="stock"></a></p>
<p style="text-align: center;"><font size="+3">Tesla Stock</font></p>
<p>For this project, I am using a stock with a highly volatile price. I want to use a stock with a highly volatile price because I believe it will lead to more versatile and complex algorithms. I chose to train on <a href="https://finance.yahoo.com/quote/TSLA?p=TSLA&amp;.tsrc=fin-srch">$TSLA</a> because it meets my volatility requirement with a 52-week price range of $211.00 - $1027.48.</p>

<p style="text-align: center;"><font size="+3">Stock Data</font></p>
<p>The financial data consists of historical trading data and technical indicators. There are 19 variables in my dataset which serve as inputs for my ANNs. The financial data I decided to use for this research project was largely influenced by <a href="#references"><em>Improving Stock Closing Price Prediction Using Recurrent Neural Network and Technical Indicators</em></a>. The historical trading data was downloaded from yahoo finance.</p>

<p style="text-align: center;"><font size="+3">Neat-Python</font></p>

<p>I used the <a href="https://neat-python.readthedocs.io/en/latest/">neat-python</a> implementation of NEAT for this project. Neat-python is a library with tools that maintain individual genomes (artificial neural networks). These genomes are made up of genes that contain important information about the structure of the algorithm.</p>

<p style="text-align: center;"><font size="+3">Methods</font></p>

<h4>Training and Testing Data</h4>
<p>The first thing my program does is split my financial dataset into two pieces. The first 70% of the dataset becomes the training data, and the last 30% of the dataset becomes the testing data. <br /></p>

<h4>Measuring Performance</h4>
<p>The training dataset is first randomly sampled. Then the ANNs produce outputs (closing price predictions) from the sampled data. While the ANNs produce outputs, their error
is determined by subtracting their output from the expected output (closing stock price). 
<br /></p>
<h4>Speciation and Reproduction</h4>
<p>Once the ANNs have completed the training dataset, they are divided up into species based on genomic distance (this is a measure of how similar genomes are based on their structure i.e. nodes and connections). After all the ANNs have been put in a species, parents are selected to produce offspring through sexual and asexual reproduction. These offspring may be mutated and will be the next generation of ANNs. This cycle of measuring performance, speciation, selection, reproduction, and mutation will continue for <em>N</em> generations. 
<br /></p>
<h4>Testing Data</h4>
<p>After <em>N</em> generations, my code checks to see if the best ANN meets my performance threshold.</p>

<p>•	If the best performing ANN does not meet this threshold, the ANNs continue to run on the training data for <em>N</em> generations.</p>

<p>•	When the best ANN meets this performance threshold, it is run on the entire testing dataset. The outputs (stock price predictions) are recorded, and my code checks to see if the best neural network is good enough to stop running the program.</p>

<p style="text-align: center;"><font size="+3">Exploratory Runs</font></p>
<p>After I wrote the initial code, I ran several exploratory runs to make sure everything worked properly. These exploratory runs also helped me understand how NEAT behaves in different conditions. Before using financial data, I used numbers from a sine wave as inputs to see if my ANNs could predict the sine wave. The ANNs were successful and I made several adjustments to my code. One of these adjustments was lowering the species compatibility threshold because I noticed that most species collapsed after a few hundred generations.</p>

<p style="text-align: center;"><font size="+3">Current Experiments</font></p>
<p>My first experiment was to see if my ANNs could predict the closing stock price using normalized data. I graphed the best ANNs testing data predictions for all 50 runs. As a baseline, I chose to use yesterday’s closing price as the current day’s prediction. The baseline will be represented with red dots in Figures 3 through 5.</p>

<div class="img">
    <img class="col three" src="/al-folio/assets/img/graph1.PNG" />
</div>
<p style="text-align: center;"><i><strong>Figure 3</strong> Artificial neural network testing data results after 15,000 generations for all 50 runs.</i></p>
<p><br /></p>

<p>When looking at Figure 3, it is clear the ANNs could not perform as well as the baseline. I started to investigate what could cause this problem and came up with several hypotheses. My first hypothesis was that only giving the ANNs one Activation Function hindered their ability to predict large numbers. After giving the ANNs access to all Activation Functions in the neat-python library, this was the result:</p>

<div class="img">
    <img class="col three" src="/al-folio/assets/img/graph2.PNG" />
</div>
<p style="text-align: center;"><i><strong>Figure 4</strong> Artificial neural network testing data results after 20,000 generations for all 50 runs. </i></p>
<p><br /></p>

<p>compared to the previous experiment, the ANNs clearly performed much better with access to all Activation Functions. Despite this improvement, the ANNs still failed to outperform the baseline as shown in Figure 4. My second hypothesis was that normalizing the data made it difficult for the ANNs to identify important inputs. Here is the result of giving the ANNs access to all Activation Functions and running them on un-normalized data:</p>

<div class="img">
    <img class="col three" src="/al-folio/assets/img/graph3.PNG" />
</div>
<p style="text-align: center;"><i><strong>Figure 5</strong> Artificial neural network testing data results after 15000 generations for all 50 runs. The horizontal line added to the residual plot shows the average error for my baseline</i></p>
<p><br /></p>

<p>When looking at Figure 5, it is now obvious that my ANNs are doing as well as the baseline. Although the baseline is being met, the ANNs are using the baseline strategy and struggling to improve.</p>

<p style="text-align: center;"><font size="+3">Future Work</font></p>
<p>This project has come quite far, and I have plenty of ideas for the direction of this research project. For future experiments, I plan on experimenting with other selection schemes. I would also like to switch from feed forward to recurrent neural networks.</p>

<p style="text-align: center;"><font size="+3">References</font></p>
<p>Stanley, K. O., &amp; Miikkulainen, R. (2002, May). Efficient evolution of neural network topologies. In Proceedings of the 2002 Congress on Evolutionary Computation. CEC’02 (Cat. No. 02TH8600) (Vol. 2, pp. 1757-1762). IEEE.
<br /></p>

<p>Gao, T., &amp; Chai, Y. (2018). Improving stock closing price prediction using recurrent neural network and technical indicators. Neural computation, 30(10), 2833-2854.</p>

<p><a name="references"></a></p>]]></content><author><name></name></author><category term="Machine-Learning" /><category term="Finance" /><summary type="html"><![CDATA[Introduction I began my current research project with a simple question: can I evolve an artificial neural network (ANN) to accurately predict the closing price of a stock?]]></summary></entry></feed>