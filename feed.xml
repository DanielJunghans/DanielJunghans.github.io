<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://danieljunghans.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://danieljunghans.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2022-09-13T02:54:45+00:00</updated><id>https://danieljunghans.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://danieljunghans.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://danieljunghans.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://danieljunghans.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Exploring scikit-learn</title><link href="https://danieljunghans.github.io/blog/2020/SCIKITLearn/" rel="alternate" type="text/html" title="Exploring scikit-learn"/><published>2020-08-28T02:34:00+00:00</published><updated>2020-08-28T02:34:00+00:00</updated><id>https://danieljunghans.github.io/blog/2020/SCIKITLearn</id><content type="html" xml:base="https://danieljunghans.github.io/blog/2020/SCIKITLearn/"><![CDATA[<p style="text-align: center;"><font size="+3">Introduction</font></p> <p>For my current NEAT project, I read the paper <a href="https://ieeexplore.ieee.org/abstract/document/8473214?casa_token=mA1Va18Dm6kAAAAA:v_6_aQSag5JUXPvV3uPm-BYIVUfWLtCD5HZFDXopj5UUDriA460pLKGfCr99nKgQEYCw8a-GAQ"><em>A Comparative Study of Supervised Machine Learning Algorithms for Stock Market Trend Prediction</em></a> to learn how others approached stock market prediction using supervised machine learning algorithms. This paper compared the accuracy between Support Vector Machine, Random Forest, K-Nearest Neighbor, Naive Bayes, and SoftMax algorithms. After reading about these different algorithms, I took it upon myself to learn more about the scikit-learn python library. I am learning how to use scikit-learn because it will give me the ability to implement some the algorithms outlined in the paper. The algorithms I have covered so far include: <br/> <a href="#RandomForest">• Random Forest</a><br/> <a href="#KNN">• K-Nearest Neighbors</a><br/> <a href="#SVM">• Support Vector Machine</a><br/></p> <p><a name="RandomForest"></a></p> <p style="text-align: center;"><font size="+3">Random Forest</font></p> <p>The first algorithm I sought to better understand was the Random Forest algorithm. This algorithm is made up of multiple decision trees. A decision tree learns simple decision rules from the data and produces a output. Each tree is used on a different sub sample of the dataset and averaging is used to improve accuracy. The decision trees all “vote” by producing outputs, and whatever the most popular output is becomes the algorithms output.</p> <p>I first imported my data as a Pandas data frame and split it up into a training section and a testing section. I then created a Random Forest algorithm using the scikit-learn random forest classifier. After running the algorithm a few times, I noticed that the accuracy was unusually high. After some investigation, I discovered that the algorithms were predicting the stock price movement for the current day instead of one day in the future. After fixing the problem, the accuracy on the testing dataset stayed around 50%. Here is the code that I wrote:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="nn">csv</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>

<span class="c1">#this opens the file and puts the data in a pandas dataframe
</span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'RandomForest.csv'</span><span class="p">)</span>
<span class="n">data</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>

<span class="c1">#setting the size of the training dataset
</span><span class="n">training_size</span> <span class="o">=</span> <span class="p">.</span><span class="mi">7</span>
<span class="n">split</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">*</span><span class="p">.</span><span class="mi">7</span><span class="p">)</span>

<span class="c1">#Identifying all of the inputs and the expected output
</span><span class="n">X</span><span class="o">=</span><span class="n">data</span><span class="p">[[</span><span class="s">'Open'</span><span class="p">,</span><span class="s">'High'</span><span class="p">,</span><span class="s">'Low'</span><span class="p">,</span><span class="s">'Close'</span><span class="p">,</span><span class="s">'Volume'</span><span class="p">,</span><span class="s">'Accumulation Distribution Line'</span><span class="p">,</span><span class="s">'MACD'</span><span class="p">,</span><span class="s">'Chaikan Oscillator (CHO)'</span><span class="p">,</span><span class="s">'Highest closing price (5 days)'</span><span class="p">,</span>
<span class="s">'Lowest closing price (days)'</span><span class="p">,</span><span class="s">'Stochastic %K (5 days)'</span><span class="p">,</span><span class="s">'%D'</span><span class="p">,</span><span class="s">'Volume Price Trend (VPT)'</span><span class="p">,</span><span class="s">'Williams %R (14 days)'</span><span class="p">,</span><span class="s">'Relative Strength Index'</span><span class="p">,</span><span class="s">'Momentum (10 days)'</span><span class="p">,</span>
<span class="s">'Price rate of change (PROC)'</span><span class="p">,</span><span class="s">'Volume rate of change (VROC)'</span><span class="p">,</span><span class="s">'On Balance Volume (OBV)'</span><span class="p">]]</span>
<span class="n">y</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s">'Outputs'</span><span class="p">]</span>

<span class="c1">#splitting the dataframe into a training and testing dataset
</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="n">split</span><span class="p">]</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">split</span><span class="p">:]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="n">split</span><span class="p">]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">split</span><span class="p">:]</span>

<span class="c1">#create a the random forest with 500 decision trees
</span><span class="n">clf</span><span class="o">=</span><span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span> 

<span class="c1">#train the model on the training data
</span><span class="n">clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="c1">#making predictions on the testing data
</span><span class="n">y_pred</span><span class="o">=</span><span class="n">clf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1">#printing the testing accuracy
</span><span class="k">print</span><span class="p">(</span><span class="s">"Accuracy:"</span><span class="p">,</span><span class="n">metrics</span><span class="p">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></figure> <p><a name="KNN"></a></p> <p style="text-align: center;"><font size="+3">K-Nearest Neighbors</font></p> <p>The next algorithm I studied was the K-Nearest Neighbors algorithm. The nearest neighbor method looks for the most similar trading days. The predefined number of the most similar trading days become the nearest neighbors. Just like the decision trees in the Random Forest algorithm, the nearest neighbors “vote”. Instead of using decision trees to produce an output, the K-Nearest Neighbor algorithm uses the actual expected output for the most similar trading days. The most popular output becomes of the neighbors becomes the algorithms output. If 5 of the nearest neighbors had the closing stock price go up and 3 neighbors had the stock price go down, the prediction will be that the stock price goes up.</p> <p>While studying this algorithm, I learned that K-Nearest Neighbor algorithms performs better with a lower number of features/inputs. To reduce the size of my dataset, I first normalized it with the standard scaler tool from scikit-learn. I then used PCA to shrink the number of inputs in my dataset down to two while preserving the variance in the dataset. Here is my K-Nearest Neighbors code:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="nn">csv</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1">#this opens the CSV File
</span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'RandomForest.csv'</span><span class="p">)</span>
<span class="n">data</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>

<span class="c1">#preprocessing and normalizing
</span><span class="n">Features</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Open'</span><span class="p">,</span><span class="s">'High'</span><span class="p">,</span><span class="s">'Low'</span><span class="p">,</span><span class="s">'Close'</span><span class="p">,</span><span class="s">'Volume'</span><span class="p">,</span><span class="s">'Accumulation Distribution Line'</span><span class="p">,</span><span class="s">'MACD'</span><span class="p">,</span><span class="s">'Chaikan Oscillator (CHO)'</span><span class="p">,</span><span class="s">'Highest closing price (5 days)'</span><span class="p">,</span>
<span class="s">'Lowest closing price (days)'</span><span class="p">,</span><span class="s">'Stochastic %K (5 days)'</span><span class="p">,</span><span class="s">'%D'</span><span class="p">,</span><span class="s">'Volume Price Trend (VPT)'</span><span class="p">,</span><span class="s">'Williams %R (14 days)'</span><span class="p">,</span><span class="s">'Relative Strength Index'</span><span class="p">,</span><span class="s">'Momentum (10 days)'</span><span class="p">,</span>
<span class="s">'Price rate of change (PROC)'</span><span class="p">,</span><span class="s">'Volume rate of change (VROC)'</span><span class="p">,</span><span class="s">'On Balance Volume (OBV)'</span><span class="p">]</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">Features</span><span class="p">].</span><span class="n">values</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="s">'Outputs'</span><span class="p">]].</span><span class="n">values</span>

<span class="c1">#normalizing the dataset
</span><span class="n">X</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">().</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1">#performing PCA
</span><span class="n">PCA</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">Components</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">ComponentDf</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">Components</span><span class="p">)</span>

<span class="c1">#setting the size of the training set
</span><span class="n">training_size</span> <span class="o">=</span> <span class="p">.</span><span class="mi">8</span>
<span class="n">split</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">*</span><span class="n">training_size</span><span class="p">)</span>

<span class="c1">#splitting up the dataset
</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">ComponentDf</span><span class="p">[:</span><span class="n">split</span><span class="p">])</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">ComponentDf</span><span class="p">[</span><span class="n">split</span><span class="p">:])</span>
<span class="n">Y_train</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">Y</span><span class="p">[:</span><span class="n">split</span><span class="p">])</span>
<span class="n">Y_test</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">Y</span><span class="p">[</span><span class="n">split</span><span class="p">:])</span>

<span class="c1">#create the K Nearest Neighbor Algorithm and running the algorithm
</span><span class="n">clf</span><span class="o">=</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="n">ravel</span><span class="p">())</span>
<span class="n">Y_pred</span><span class="o">=</span><span class="n">clf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1">#printing the testing accuracy 
</span><span class="k">print</span><span class="p">(</span><span class="s">"Accuracy:"</span><span class="p">,</span><span class="n">metrics</span><span class="p">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>After performing principal component analysis on my dataset, I graphed the principal components to better understand the data. After creating the graph, it became clear why the accuracy of my K-Nearest Neighbor algorithm stayed around 50%. Both expected outputs (Stock price going up or down) are clustered together.</p> <div class="img"> <img class="col three" src="/assets/img/Component.PNG"/> </div> <p><a name="SVM"></a></p> <p style="text-align: center;"><font size="+3">Support Vector Machine</font></p> <p>A Support Vector Machine is a machine learning tool that uses a hyperplane to define data points. Instead of looking at the nearest neighbors like a KNN, I like to think that Support Vector Machines split up data points into different neighborhoods. For example, if we use the two components from my PCA analysis, a SVM will create a one dimensional hyperplane splitting up the data into two “neighborhoods”. The hyperplane will create decision boundaries that maximize the margins from both expected outputs. The graph below shows the data points and the decision boundaries created by my SVM.</p> <div class="img"> <img class="col three" src="/assets/img/graph10.png"/> </div> <p>Any data point that falls into the blue background gets categorized as a 0 (closing stock price goes down). To create this graph, I used a Support Vector Machine with a RBF kernel. Kernel functions allow SVMs to create hyperplanes in high dimensional data without having to calculate the coordinates of the data in that space. Here is my Support Vector Machine Code:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
</pre></td><td class="code"><pre><span class="c1">#support vector machine
</span>
<span class="kn">import</span> <span class="nn">csv</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1">#this opens the CSV File
</span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'RandomForest.csv'</span><span class="p">)</span>
<span class="n">data</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>

<span class="c1">#setting the size of the training dataset
</span><span class="n">training_size</span> <span class="o">=</span> <span class="p">.</span><span class="mi">7</span>
<span class="n">split</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">*</span><span class="p">.</span><span class="mi">7</span><span class="p">)</span>

<span class="c1">#Identifying all of the inputs and the expected output
</span><span class="n">X</span><span class="o">=</span><span class="n">data</span><span class="p">[[</span><span class="s">'Open'</span><span class="p">,</span><span class="s">'High'</span><span class="p">,</span><span class="s">'Low'</span><span class="p">,</span><span class="s">'Close'</span><span class="p">,</span><span class="s">'Volume'</span><span class="p">,</span><span class="s">'Accumulation Distribution Line'</span><span class="p">,</span><span class="s">'MACD'</span><span class="p">,</span><span class="s">'Chaikan Oscillator (CHO)'</span><span class="p">,</span><span class="s">'Highest closing price (5 days)'</span><span class="p">,</span>
<span class="s">'Lowest closing price (days)'</span><span class="p">,</span><span class="s">'Stochastic %K (5 days)'</span><span class="p">,</span><span class="s">'%D'</span><span class="p">,</span><span class="s">'Volume Price Trend (VPT)'</span><span class="p">,</span><span class="s">'Williams %R (14 days)'</span><span class="p">,</span><span class="s">'Relative Strength Index'</span><span class="p">,</span><span class="s">'Momentum (10 days)'</span><span class="p">,</span>
<span class="s">'Price rate of change (PROC)'</span><span class="p">,</span><span class="s">'Volume rate of change (VROC)'</span><span class="p">,</span><span class="s">'On Balance Volume (OBV)'</span><span class="p">]]</span>
<span class="n">y</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s">'Outputs'</span><span class="p">]</span>

<span class="c1">#splitting the dataframe into a training and testing dataset
</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="n">split</span><span class="p">]</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">split</span><span class="p">:]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="n">split</span><span class="p">]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">split</span><span class="p">:]</span>

<span class="c1">#create the support vector machine
</span><span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="p">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">'rbf'</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">svc</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">svc</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1">#printing the testing accuracy
</span><span class="k">print</span><span class="p">(</span><span class="s">"Accuracy:"</span><span class="p">,</span><span class="n">metrics</span><span class="p">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>

<span class="c1">##################################################
#The next section transforms the dataset#
#And graphs the components with decision boundries
</span>
<span class="c1">#Normalizing the dataset
</span><span class="n">X</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">().</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1">#performing PCA
</span><span class="n">PCA</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">Components</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">ComponentDf</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">Components</span><span class="p">,</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'principal component 1'</span><span class="p">,</span> <span class="s">'principal component 2'</span><span class="p">])</span>

<span class="c1">#transforming the dataset
</span><span class="n">X</span> <span class="o">=</span> <span class="n">ComponentDf</span><span class="p">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="n">h</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="c1"># create a mesh to plot in
</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">].</span><span class="nb">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">].</span><span class="nb">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">].</span><span class="nb">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">].</span><span class="nb">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span>
                     <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span>

<span class="c1">#create the support vector machine
</span><span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="p">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">'rbf'</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1">#graphing the data
</span><span class="n">Z</span> <span class="o">=</span> <span class="n">svc</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="p">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="p">.</span><span class="n">ravel</span><span class="p">()])</span>

<span class="c1"># Put the result into a color plot
</span><span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="p">.</span><span class="n">cm</span><span class="p">.</span><span class="n">coolwarm</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

<span class="c1"># Creating the plot
</span><span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="p">.</span><span class="n">cm</span><span class="p">.</span><span class="n">coolwarm</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Principal Component 1'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Principal Component 2'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xx</span><span class="p">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">xx</span><span class="p">.</span><span class="nb">max</span><span class="p">())</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">yy</span><span class="p">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">yy</span><span class="p">.</span><span class="nb">max</span><span class="p">())</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">(())</span>
<span class="n">plt</span><span class="p">.</span><span class="n">yticks</span><span class="p">(())</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Support Vector Machine Graph'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Saving the Plot
</span><span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">"matplotlib.png"</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>]]></content><author><name></name></author><summary type="html"><![CDATA[Introduction For my current NEAT project, I read the paper A Comparative Study of Supervised Machine Learning Algorithms for Stock Market Trend Prediction to learn how others approached stock market prediction using supervised machine learning algorithms. This paper compared the accuracy between Support Vector Machine, Random Forest, K-Nearest Neighbor, Naive Bayes, and SoftMax algorithms. After reading about these different algorithms, I took it upon myself to learn more about the scikit-learn python library. I am learning how to use scikit-learn because it will give me the ability to implement some the algorithms outlined in the paper. The algorithms I have covered so far include: • Random Forest • K-Nearest Neighbors • Support Vector Machine Random Forest The first algorithm I sought to better understand was the Random Forest algorithm. This algorithm is made up of multiple decision trees. A decision tree learns simple decision rules from the data and produces a output. Each tree is used on a different sub sample of the dataset and averaging is used to improve accuracy. The decision trees all “vote” by producing outputs, and whatever the most popular output is becomes the algorithms output. I first imported my data as a Pandas data frame and split it up into a training section and a testing section. I then created a Random Forest algorithm using the scikit-learn random forest classifier. After running the algorithm a few times, I noticed that the accuracy was unusually high. After some investigation, I discovered that the algorithms were predicting the stock price movement for the current day instead of one day in the future. After fixing the problem, the accuracy on the testing dataset stayed around 50%. Here is the code that I wrote: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import csv import pandas as pd from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn import metrics from sklearn.metrics import classification_report #this opens the file and puts the data in a pandas dataframe data=pd.read_csv('RandomForest.csv') data.head() #setting the size of the training dataset training_size=.7 split=int(len(data)*.7) #Identifying all of the inputs and the expected output X=data[['Open','High','Low','Close','Volume','Accumulation Distribution Line','MACD','Chaikan Oscillator (CHO)','Highest closing price (5 days)', 'Lowest closing price (days)','Stochastic %K (5 days)','%D','Volume Price Trend (VPT)','Williams %R (14 days)','Relative Strength Index','Momentum (10 days)', 'Price rate of change (PROC)','Volume rate of change (VROC)','On Balance Volume (OBV)']] y=data['Outputs'] #splitting the dataframe into a training and testing dataset X_train=X[:split] X_test=X[split:] y_train=y[:split] y_test=y[split:] #create a the random forest with 500 decision trees clf=RandomForestClassifier(n_estimators=500) #train the model on the training data clf.fit(X_train,y_train) #making predictions on the testing data y_pred=clf.predict(X_test) #printing the testing accuracy print("Accuracy:",metrics.accuracy_score(y_test, y_pred)) K-Nearest Neighbors The next algorithm I studied was the K-Nearest Neighbors algorithm. The nearest neighbor method looks for the most similar trading days. The predefined number of the most similar trading days become the nearest neighbors. Just like the decision trees in the Random Forest algorithm, the nearest neighbors “vote”. Instead of using decision trees to produce an output, the K-Nearest Neighbor algorithm uses the actual expected output for the most similar trading days. The most popular output becomes of the neighbors becomes the algorithms output. If 5 of the nearest neighbors had the closing stock price go up and 3 neighbors had the stock price go down, the prediction will be that the stock price goes up. While studying this algorithm, I learned that K-Nearest Neighbor algorithms performs better with a lower number of features/inputs. To reduce the size of my dataset, I first normalized it with the standard scaler tool from scikit-learn. I then used PCA to shrink the number of inputs in my dataset down to two while preserving the variance in the dataset. Here is my K-Nearest Neighbors code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import csv import pandas as pd from sklearn import metrics from sklearn.metrics import classification_report from sklearn.neighbors import KNeighborsClassifier from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler #this opens the CSV File data=pd.read_csv('RandomForest.csv') data.head() #preprocessing and normalizing Features=['Open','High','Low','Close','Volume','Accumulation Distribution Line','MACD','Chaikan Oscillator (CHO)','Highest closing price (5 days)', 'Lowest closing price (days)','Stochastic %K (5 days)','%D','Volume Price Trend (VPT)','Williams %R (14 days)','Relative Strength Index','Momentum (10 days)', 'Price rate of change (PROC)','Volume rate of change (VROC)','On Balance Volume (OBV)'] X=data.loc[:, Features].values Y=data.loc[:,['Outputs']].values #normalizing the dataset X=StandardScaler().fit_transform(X) #performing PCA PCA=PCA(n_components=2) Components=PCA.fit_transform(X) ComponentDf=pd.DataFrame(data=Components) #setting the size of the training set training_size=.8 split=int(len(data)*training_size) #splitting up the dataset X_train=pd.DataFrame(data=ComponentDf[:split]) X_test=pd.DataFrame(data=ComponentDf[split:]) Y_train=pd.DataFrame(data=Y[:split]) Y_test=pd.DataFrame(data=Y[split:]) #create the K Nearest Neighbor Algorithm and running the algorithm clf=KNeighborsClassifier(n_neighbors=5) clf.fit(X_train, Y_train.values.ravel()) Y_pred=clf.predict(X_test) #printing the testing accuracy print("Accuracy:",metrics.accuracy_score(Y_test, Y_pred)) After performing principal component analysis on my dataset, I graphed the principal components to better understand the data. After creating the graph, it became clear why the accuracy of my K-Nearest Neighbor algorithm stayed around 50%. Both expected outputs (Stock price going up or down) are clustered together. Support Vector Machine A Support Vector Machine is a machine learning tool that uses a hyperplane to define data points. Instead of looking at the nearest neighbors like a KNN, I like to think that Support Vector Machines split up data points into different neighborhoods. For example, if we use the two components from my PCA analysis, a SVM will create a one dimensional hyperplane splitting up the data into two “neighborhoods”. The hyperplane will create decision boundaries that maximize the margins from both expected outputs. The graph below shows the data points and the decision boundaries created by my SVM. Any data point that falls into the blue background gets categorized as a 0 (closing stock price goes down). To create this graph, I used a Support Vector Machine with a RBF kernel. Kernel functions allow SVMs to create hyperplanes in high dimensional data without having to calculate the coordinates of the data in that space. Here is my Support Vector Machine Code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 #support vector machine import csv import pandas as pd from sklearn import metrics from sklearn.metrics import classification_report from sklearn import svm from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler import matplotlib.pyplot as plt import numpy as np #this opens the CSV File data=pd.read_csv('RandomForest.csv') data.head() #setting the size of the training dataset training_size=.7 split=int(len(data)*.7) #Identifying all of the inputs and the expected output X=data[['Open','High','Low','Close','Volume','Accumulation Distribution Line','MACD','Chaikan Oscillator (CHO)','Highest closing price (5 days)', 'Lowest closing price (days)','Stochastic %K (5 days)','%D','Volume Price Trend (VPT)','Williams %R (14 days)','Relative Strength Index','Momentum (10 days)', 'Price rate of change (PROC)','Volume rate of change (VROC)','On Balance Volume (OBV)']] y=data['Outputs'] #splitting the dataframe into a training and testing dataset X_train=X[:split] X_test=X[split:] y_train=y[:split] y_test=y[split:] #create the support vector machine svc=svm.SVC(kernel='rbf', C=1.0) svc.fit(X_train, y_train) y_pred=svc.predict(X_test) #printing the testing accuracy print("Accuracy:",metrics.accuracy_score(y_test, y_pred)) ################################################## #The next section transforms the dataset# #And graphs the components with decision boundries #Normalizing the dataset X=StandardScaler().fit_transform(X) #performing PCA PCA=PCA(n_components=2) Components=PCA.fit_transform(X) ComponentDf=pd.DataFrame(data=Components,columns = ['principal component 1', 'principal component 2']) #transforming the dataset X=ComponentDf.to_numpy() y=y.ravel() h=0.2 # create a mesh to plot in x_min, x_max=X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max=X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy=np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) #create the support vector machine svc=svm.SVC(kernel='rbf', C=1.0).fit(X, y) #graphing the data Z=svc.predict(np.c_[xx.ravel(), yy.ravel()]) # Put the result into a color plot Z=Z.reshape(xx.shape) plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8) # Creating the plot plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm) plt.xlabel('Principal Component 1') plt.ylabel('Principal Component 2') plt.xlim(xx.min(), xx.max()) plt.ylim(yy.min(), yy.max()) plt.xticks(()) plt.yticks(()) plt.title('Support Vector Machine Graph') plt.legend() # Saving the Plot plt.savefig("matplotlib.png")]]></summary></entry><entry><title type="html">2020 Summer Reflection</title><link href="https://danieljunghans.github.io/blog/2020/Reflection/" rel="alternate" type="text/html" title="2020 Summer Reflection"/><published>2020-08-20T01:44:00+00:00</published><updated>2020-08-20T01:44:00+00:00</updated><id>https://danieljunghans.github.io/blog/2020/Reflection</id><content type="html" xml:base="https://danieljunghans.github.io/blog/2020/Reflection/"><![CDATA[<p style="text-align: center;"><font size="+3">Reflection</font></p> <p>Looking back, I am very satisfied with the work I have done this summer. I started off the summer by working on my NEAT project and taking the course Financial Management 311. The beginning of this course focused on how to construct an income statement, balance sheet, and statement of cash flows. The next big concept this course covered was the time value of money and compound interest. I learned about the time value of money by doing present and future value calculations in excel. The last section of this course focused on bonds and capital budgeting decisions. I first learned about bonds and bond ratings, but quickly began performing NPV, payback period, discounted payback period, and IRR calculations.</p> <p><br/> Financial Management 311 was very informative and a fun summer class. This course greatly expanded my knowledge of financial instruments and capital budgeting decisions. Although this class consumed a lot of my time, I was still working on other things. I continued my NEAT project during this time and was able to finish my website.</p> <p><br/> After finishing Financial Management 311, I started taking Principles of Management Accounting 202. This course started by helping me understand the differences between financial and management accounting. Next, I began to learn about different cost classifications, and the basic manufacturing cost categories. With this new knowledge of cost classifications, I started to create traditional and contribution format income statements. Moving on from contribution income statements, I learned how to calculate the predetermined overhead rate and the different ways overhead is allocated. After learning about manufacturing overhead, I began to learn about costing methods and the flow of costs from raw materials to the cost of goods manufactured. Finally, the last few chapters for this course focused on Cost Volume Profit analysis, break even analysis, ROI, and Residual Income calculations.</p> <p style="text-align: center;"><font size="+3">Summer Research Update</font></p> <p>Throughout the summer, I continued to work on my NEAT project while taking the summer courses described above. After reading several papers about algorithms predicting stock price movement, I decided to change my algorithm so it is predicting whether the closing stock price will go up or down. Previously my algorithm predicting the closing price for the next day, but now it just predicts whether the price is going up or down. After changing the expected outputs, here was the algorithm accuracy for all models:</p> <div class="img"> <img class="col three" src="/assets/img/graph9.PNG"/> </div> <p>After running NEAT I created a random forrest algorithm using the python library <a href="https://scikit-learn.org/stable/">scikit learn</a>. The new random forrest algorithm performed very similiarly to my NEAT. For the next few weeks, I plan on becoming more familiar with the scikit learn library.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Reflection Looking back, I am very satisfied with the work I have done this summer. I started off the summer by working on my NEAT project and taking the course Financial Management 311. The beginning of this course focused on how to construct an income statement, balance sheet, and statement of cash flows. The next big concept this course covered was the time value of money and compound interest. I learned about the time value of money by doing present and future value calculations in excel. The last section of this course focused on bonds and capital budgeting decisions. I first learned about bonds and bond ratings, but quickly began performing NPV, payback period, discounted payback period, and IRR calculations. Financial Management 311 was very informative and a fun summer class. This course greatly expanded my knowledge of financial instruments and capital budgeting decisions. Although this class consumed a lot of my time, I was still working on other things. I continued my NEAT project during this time and was able to finish my website. After finishing Financial Management 311, I started taking Principles of Management Accounting 202. This course started by helping me understand the differences between financial and management accounting. Next, I began to learn about different cost classifications, and the basic manufacturing cost categories. With this new knowledge of cost classifications, I started to create traditional and contribution format income statements. Moving on from contribution income statements, I learned how to calculate the predetermined overhead rate and the different ways overhead is allocated. After learning about manufacturing overhead, I began to learn about costing methods and the flow of costs from raw materials to the cost of goods manufactured. Finally, the last few chapters for this course focused on Cost Volume Profit analysis, break even analysis, ROI, and Residual Income calculations. Summer Research Update Throughout the summer, I continued to work on my NEAT project while taking the summer courses described above. After reading several papers about algorithms predicting stock price movement, I decided to change my algorithm so it is predicting whether the closing stock price will go up or down. Previously my algorithm predicting the closing price for the next day, but now it just predicts whether the price is going up or down. After changing the expected outputs, here was the algorithm accuracy for all models: After running NEAT I created a random forrest algorithm using the python library scikit learn. The new random forrest algorithm performed very similiarly to my NEAT. For the next few weeks, I plan on becoming more familiar with the scikit learn library.]]></summary></entry><entry><title type="html">NEAT Project</title><link href="https://danieljunghans.github.io/blog/2020/NEAT/" rel="alternate" type="text/html" title="NEAT Project"/><published>2020-06-18T06:13:00+00:00</published><updated>2020-06-18T06:13:00+00:00</updated><id>https://danieljunghans.github.io/blog/2020/NEAT</id><content type="html" xml:base="https://danieljunghans.github.io/blog/2020/NEAT/"><![CDATA[<p style="text-align: center;"><font size="+3">Introduction</font></p> <p>I began my current research project with a simple question: can I evolve an artificial neural network (ANN) to accurately predict the closing price of a stock?</p> <p style="text-align: center;"><font size="+3">Artificial Neural Networks</font></p> <p>ANNs are inspired by biological brains and are made up of densely interconnected nodes. Instead of neurons and synapses, ANNs utilize artificial neurons (nodes) and connections.</p> <div class="img"> <img class="col three" src="/assets/img/neuralnetwork.png"/> </div> <p style="text-align: center;"><i><strong>Figure 1</strong> Visual representation of an artificial neural network. The circles represent nodes and the lines connecting the nodes are connections. Nodes change their outputs based on the values received from connections. </i></p> <p><br/></p> <h4>Input Layer</h4> <p>All ANNs are made up of 3 types of layers. The first layer is the input layer shown in Figure 1. This is the initial data that is read by the neural networks. For this project, the input layer will consist of historical trading data. To learn about the inputs I am using, scroll down to the <a href="#stock">Stock Data section</a>.</p> <h4>Connections</h4> <p>Connections transmit signals in the form of real numbers and each connection has its own weight. The purpose of connection weights is to either strengthen or weaken the signal that is being transmitted. ANNs learn by adjusting their connection weights. As ANNs learn, important connections will develop larger weights as weights for unimportant connections become smaller.</p> <h4>Hidden Layers</h4> <p>The hidden layers come after the input layer and is where all the computation is done. The artificial neural network in Figure 1 has two hidden layers made up of four nodes in each layer. The nodes that are in the hidden layers must find the sum of all input signals multiplied by their corresponding weights, add a node bias, and plug the result in an Activation Function.</p> <div class="img"> <img class="col three" src="/assets/img/node.png"/> </div> <p style="text-align: center;"><i><strong>Figure 2</strong> visual representation of a node. The large circle represents the node and shows the addition of the bias to the summation of the inputs multiplied by the connection weights. The value the node calculates is then plugged into a non-linear activation function and the result is the node output. </i></p> <p><br/> An Activation Function is a non-linear function used by nodes to produce an output. A common example of an Activation Function would be the sine function. The node bias shown in Figure 2 is a value added to the sum of all input signals multiplied by their weights. The purpose of adding a node bias is to move the Activation Function. The bias acts very similarly to the constant b in the linear function: <em>y=ax+b</em>. When the constant <em>b</em> is changed, the line will move up or down changing the <em>y-axis</em>. Moving an Activation Function greatly impacts the output of a node which may be useful in the learning process.</p> <h4>Output Layer</h4> <p>The last layer for Figure 1 is the output layer. The output layer is that last set of nodes that produce outputs. My ANNs will only have one node in the output layer. This last node will be responsible for outputting the closing price prediction.</p> <p style="text-align: center;"><font size="+3">NEAT</font></p> <p>While researching ANNs, I was introduced to the <a href="#references">Neural Evolution of Augmenting Topologies</a> (NEAT). NEAT was developed by Dr. Kenneth O. Stanely and is a method for evolving the structure ANNs. Unlike traditional ANNs that only adjust connection weights, NEAT gives ANNS the ability to change nodes, weights, and connections. The NEAT approach allows ANNS to add or delete connections and nodes. Stock market data is extremely noisy and allowing the structure of my ANNs to change may give them a competitive advantage.</p> <p><a name="stock"></a></p> <p style="text-align: center;"><font size="+3">Tesla Stock</font></p> <p>For this project, I am using a stock with a highly volatile price. I want to use a stock with a highly volatile price because I believe it will lead to more versatile and complex algorithms. I chose to train on <a href="https://finance.yahoo.com/quote/TSLA?p=TSLA&amp;.tsrc=fin-srch">$TSLA</a> because it meets my volatility requirement with a 52-week price range of $211.00 - $1027.48.</p> <p style="text-align: center;"><font size="+3">Stock Data</font></p> <p>The financial data consists of historical trading data and technical indicators. There are 19 variables in my dataset which serve as inputs for my ANNs. The financial data I decided to use for this research project was largely influenced by <a href="#references"><em>Improving Stock Closing Price Prediction Using Recurrent Neural Network and Technical Indicators</em></a>. The historical trading data was downloaded from yahoo finance.</p> <p style="text-align: center;"><font size="+3">Neat-Python</font></p> <p>I used the <a href="https://neat-python.readthedocs.io/en/latest/">neat-python</a> implementation of NEAT for this project. Neat-python is a library with tools that maintain individual genomes (artificial neural networks). These genomes are made up of genes that contain important information about the structure of the algorithm.</p> <p style="text-align: center;"><font size="+3">Methods</font></p> <h4>Training and Testing Data</h4> <p>The first thing my program does is split my financial dataset into two pieces. The first 70% of the dataset becomes the training data, and the last 30% of the dataset becomes the testing data. <br/></p> <h4>Measuring Performance</h4> <p>The training dataset is first randomly sampled. Then the ANNs produce outputs (closing price predictions) from the sampled data. While the ANNs produce outputs, their error is determined by subtracting their output from the expected output (closing stock price). <br/></p> <h4>Speciation and Reproduction</h4> <p>Once the ANNs have completed the training dataset, they are divided up into species based on genomic distance (this is a measure of how similar genomes are based on their structure i.e. nodes and connections). After all the ANNs have been put in a species, parents are selected to produce offspring through sexual and asexual reproduction. These offspring may be mutated and will be the next generation of ANNs. This cycle of measuring performance, speciation, selection, reproduction, and mutation will continue for <em>N</em> generations. <br/></p> <h4>Testing Data</h4> <p>After <em>N</em> generations, my code checks to see if the best ANN meets my performance threshold.</p> <p>• If the best performing ANN does not meet this threshold, the ANNs continue to run on the training data for <em>N</em> generations.</p> <p>• When the best ANN meets this performance threshold, it is run on the entire testing dataset. The outputs (stock price predictions) are recorded, and my code checks to see if the best neural network is good enough to stop running the program.</p> <p style="text-align: center;"><font size="+3">Exploratory Runs</font></p> <p>After I wrote the initial code, I ran several exploratory runs to make sure everything worked properly. These exploratory runs also helped me understand how NEAT behaves in different conditions. Before using financial data, I used numbers from a sine wave as inputs to see if my ANNs could predict the sine wave. The ANNs were successful and I made several adjustments to my code. One of these adjustments was lowering the species compatibility threshold because I noticed that most species collapsed after a few hundred generations.</p> <p style="text-align: center;"><font size="+3">Current Experiments</font></p> <p>My first experiment was to see if my ANNs could predict the closing stock price using normalized data. I graphed the best ANNs testing data predictions for all 50 runs. As a baseline, I chose to use yesterday’s closing price as the current day’s prediction. The baseline will be represented with red dots in Figures 3 through 5.</p> <div class="img"> <img class="col three" src="/assets/img/graph1.PNG"/> </div> <p style="text-align: center;"><i><strong>Figure 3</strong> Artificial neural network testing data results after 15,000 generations for all 50 runs.</i></p> <p><br/></p> <p>When looking at Figure 3, it is clear the ANNs could not perform as well as the baseline. I started to investigate what could cause this problem and came up with several hypotheses. My first hypothesis was that only giving the ANNs one Activation Function hindered their ability to predict large numbers. After giving the ANNs access to all Activation Functions in the neat-python library, this was the result:</p> <div class="img"> <img class="col three" src="/assets/img/graph2.PNG"/> </div> <p style="text-align: center;"><i><strong>Figure 4</strong> Artificial neural network testing data results after 20,000 generations for all 50 runs. </i></p> <p><br/></p> <p>compared to the previous experiment, the ANNs clearly performed much better with access to all Activation Functions. Despite this improvement, the ANNs still failed to outperform the baseline as shown in Figure 4. My second hypothesis was that normalizing the data made it difficult for the ANNs to identify important inputs. Here is the result of giving the ANNs access to all Activation Functions and running them on un-normalized data:</p> <div class="img"> <img class="col three" src="/assets/img/graph3.PNG"/> </div> <p style="text-align: center;"><i><strong>Figure 5</strong> Artificial neural network testing data results after 15000 generations for all 50 runs. The horizontal line added to the residual plot shows the average error for my baseline</i></p> <p><br/></p> <p>When looking at Figure 5, it is now obvious that my ANNs are doing as well as the baseline. Although the baseline is being met, the ANNs are using the baseline strategy and struggling to improve.</p> <p style="text-align: center;"><font size="+3">Future Work</font></p> <p>This project has come quite far, and I have plenty of ideas for the direction of this research project. For future experiments, I plan on experimenting with other selection schemes. I would also like to switch from feed forward to recurrent neural networks.</p> <p style="text-align: center;"><font size="+3">References</font></p> <p>Stanley, K. O., &amp; Miikkulainen, R. (2002, May). Efficient evolution of neural network topologies. In Proceedings of the 2002 Congress on Evolutionary Computation. CEC’02 (Cat. No. 02TH8600) (Vol. 2, pp. 1757-1762). IEEE. <br/></p> <p>Gao, T., &amp; Chai, Y. (2018). Improving stock closing price prediction using recurrent neural network and technical indicators. Neural computation, 30(10), 2833-2854.</p> <p><a name="references"></a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Introduction I began my current research project with a simple question: can I evolve an artificial neural network (ANN) to accurately predict the closing price of a stock?]]></summary></entry></feed>