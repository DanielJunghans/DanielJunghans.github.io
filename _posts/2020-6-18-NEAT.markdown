---
layout: post
title: NEAT Project Part 1
date: 2020-6-18 2:13:00-0400
description: 
comments: false
---
**page under construction**

<p style="text-align: center;"><font size="+3">Introduction</font></p>
Hello! My name is Daniel Junghans. I am an undergraduate business student that is working in Michigan State University's BEACON Center under the supervision of Dr. Charles Ofria. I began my current research project with a simple question: can I evolve an artificial neural network (ANN) to accurately predict the closing price of a stock? 

<p style="text-align: center;"><font size="+3">Artificial Neural Networks</font></p>
ANNs are inspired by biological brains and are made up of densely interconnected nodes. Instead of neurons and synapses, ANNs utilize artificial neurons (nodes) and connections.      


<div class="img">
    <img class="col three" src="{{ site.baseurl }}/assets/img/neuralnetwork.png">
</div>

<h4>Input Layer</h4>
All ANNs are made up of 3 types of layers. The first layer is the input layer. This is the initial data used by the neural networks. For this project, the input layer will consist of historical trading data. To learn about the inputs I am using for this project, scroll down to the Stock Data section. 

<h4>Connections</h4>
The lines connecting the inputs and nodes are called connections. These connections transmit signals in the form of real numbers. Each connection has its own weight. The purpose of connection weights is to either strengthen or weaken the signal that is being transmitted. ANNs learn by adjusting their connection weights. As ANNs learn, important connections will develop larger weights as weights for unimportant connections become smaller. 

<h4>Hidden Layers</h4>
The second type of layer is the hidden layers. The hidden layers come before the output and is where all the computation is done. The nodes that make up the hidden layers must find the sum of all input signals multiplied by their corresponding weights, add a node bias, and plug the result in a Activation Function.

##ADD Photo



An Activation Function is a non-linear function used by nodes in order to produce a output. A common example of a Activation Funciton would be the sine function. Node bias is a value added to the sum of all input signals multiplied by their weights. The purpose of adding a node bias is to shift the Activation Function. The bias acts very similarily to the constant b in the linear function: y = ax + b. 

##talk about when you move the constant b, the y intercept changes and the line is shifted

##briefly talk about why adding bias helps the neural networks during the learning process

<h4>Output Layer</h4>
The output layer is that last set of nodes that produce outputs. My ANNs will only have one node in the output layer. This last node will be responsible for outputing the closing price prediction.



<p style="text-align: center;"><font size="+3">NEAT</font></p>
While researching ANNs, I was introduced to the [Neural Evolution of Augmenting Topologies](https://ieeexplore.ieee.org/abstract/document/1004508?casa_token=GvSoqJe__7oAAAAA:4MxGQXXGS6P-w3srDdBVwXhW0vGG8oF9TGfN9W-RmY3o_7dS5259uk6JBSAKauHqO4Fi1gtntw) (NEAT). NEAT was developed by Dr. Kenneth O. Stanely and is a method for evolving the structure ANNs. I am using NEAT for this project because neuroevolution will help my ANNs explore the problem landscape.


NOTE mention how neat evolves the structure by adding/deleting connections/nodes .... Unlike traditional ANNs, NEAT


<p style="text-align: center;"><font size="+3">Tesla Stock</font></p>
For this project, I am using a stock with a highly volatile price. I want to use a stock with a highly volatile price because I believe it will lead to more versatile and complex algorithms. The company I am using is $TSLA because it meets my volatility requirement with a 52-week price range of $211.00 - $1027.48. 

<p style="text-align: center;"><font size="+3">Stock Data</font></p>
The financial data consists of historical trading data and technical indicators. There are 19 variables in my dataset which serve as inputs for my ANNs. The financial data I decided to use for this research project was largely influenced by [*Improving Stock Closing Price Prediction Using Recurrent Neural Network and Technical Indicators*](https://www-mitpressjournals-org.proxy1.cl.msu.edu/doi/full/10.1162/neco_a_01124).


<p style="text-align: center;"><font size="+3">Neat-Python</font></p>

I used the [neat-python](https://neat-python.readthedocs.io/en/latest/) implementation of NEAT for this project. Neat-python is a library with tools that maintain individual genomes (artificial neural networks). These genomes are made up of genes that contain important information about the structure of the algorithm.

<p style="text-align: center;"><font size="+3">Code</font></p>

<h4>Training and Testing Data</h4>
The first thing my program does is split my financial dataset into two pieces. The first 70% of the dataset becomes the training data, and the last 30% of the dataset becomes the testing data. <br />

<h4>Measuring Performance</h4>
The training dataset is first randomly sampled. Then the ANNs produce outputs (closing price predictions) from the sampled data. While the ANNs produce outputs, their error
is determined by subtracting their output from the expected output (closing stock price). 
<br />
<h4>Speciation and Reproduction</h4>
Once the ANNs have completed the training dataset, they are divided up into species based on genomic distance (this is a measure of how similar genomes are based on their structure i.e. nodes and connections). After all the ANNs have been put in a species, parents are selected to produce offspring through sexual and asexual reproduction. These offspring may be mutated and will be the next generation of ANNs. This cycle of measuring performance, speciation, selection, reproduction, and mutation will continue for N generations. 
<br />
<h4>Testing Data</h4>
After N generations, my code checks to see if the best ANN meets my performance threshold.  

•	If the best performing ANN does not meet this threshold, then the ANNs continue to run on the training data for N generations. 

•	When the best ANN meets this performance threshold, it is run on the entire testing dataset. The outputs (stock price predictions) are recorded, and my code checks to see if the best neural network is good enough to stop running the program.  


<p style="text-align: center;"><font size="+3">Exploratory Runs</font></p>
After I wrote the initial code, I ran several exploratory runs to make sure everything worked properly. These exploratory runs also helped me understand how NEAT behaves in different conditions. Before using financial data, I used numbers from a sine wave as inputs to see if my ANNs could predict the sine wave. The ANNs were successful and I made several adjustments to my code. One of these adjustments was lowering the species compatibility threshold because I noticed that most species collapsed after a few hundred generations. 

<p style="text-align: center;"><font size="+3">Current Experiments</font></p>
My first experiment was to see if my ANNs could predict the closing stock price using normalized data. I graphed the best ANNs testing data predictions for all 50 runs. As a base line, I chose to use yesterdays closing price as the current days prediction. 

<div class="img">
    <img class="col three" src="{{ site.baseurl }}/assets/img/graph1.PNG">
</div>

It is instantly clear that the ANNs could not perform as well as the baseline. I started to investigate what could cause this problem and came up with several hypotheses. My first hypothesis was that only giving the ANNs one Activation Function hindered their ability to predict large numbers. After giving the ANNs access to all Activation Functions in the neat-python library, this was the result:

<div class="img">
    <img class="col three" src="{{ site.baseurl }}/assets/img/graph2.PNG">
</div>

The ANNs clearly performed much better with access to all Activation Functions. Despite this improvement, the ANNs still failed to perform as well as the baseline. My second hypothesis was that normalizing the data made it difficult for the ANNs to identify important inputs. Here is the result of giving the ANNs access to all Activation Functions and running them on un-normalized data:

<div class="img">
    <img class="col three" src="{{ site.baseurl }}/assets/img/graph3.PNG">
</div>

My ANNs are now doing as well as the baseline. The horizontal line added to the residual plot represents the average error for my baseline. 

<p style="text-align: center;"><font size="+3">Future Work</font></p>
This project has come quite far, and I have plenty of ideas for the direction of this research project. For future experiments, I plan on experimenting with other selection schemes. I would also like to switch from feed forward to recurrent neural networks. 

